{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ChunkNorris' documentation !","text":""},{"location":"#what-is-chunknorris","title":"What is <code>chunknorris</code> ?","text":"<p>In a nutshell, <code>chunknorris</code> is a python package that aims at drastically improve the chunking of documents from various sources (HTML, PDFs, Markdown, ...) while keeping the usage of computational ressources to the minimum. </p> <p>\ud83e\uddea Try it out !</p>"},{"location":"#why-should-i-use-it","title":"Why should I use it ?","text":"<p>In the context of Retrieval Augmented Generation (RAG), an optimized chunking strategy leads to :</p> <ul> <li>Better relevancy of chunks and thus easier identification of useful chunks through more expressive embeddings.</li> <li>Less hallucinations of generation models because of superfluous information in the prompt</li> <li>Less errors because of chunks exceeding the API limits in terms of number of tokens</li> <li>Reduced cost as the prompt can have reduced size</li> </ul> <p>As of today, many packages exist with the intent of parsing documents. Though the vast majority of them :</p> <ul> <li>rely on high computational requirements</li> <li>do not provide chunks out of the box, and instead provide parsing of the documents on top of which the user has to build the chunking implementation.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>\ud83c\udfeb In this section, you may find quickstart tutorials and in-depth guides for parsing and chunking your documents in various use cases !</p>"},{"location":"examples/advanced_chunking/","title":"Influence chunking behavior","text":"In\u00a0[\u00a0]: Copied! <pre># If needed, install chunknorris\n%pip install chunknorris -q\n</pre> # If needed, install chunknorris %pip install chunknorris -q In\u00a0[7]: Copied! <pre># utility functions\ndef print_chunking_result(chunks):\n    print(f\"\\n======= Got {len(chunks)} chunks ! ========\\n\")\n    for i, chunk in enumerate(chunks):\n        print(f\"--------------------- chunk {i} ---------------------\")\n        print(chunk.get_text())\n</pre> # utility functions def print_chunking_result(chunks):     print(f\"\\n======= Got {len(chunks)} chunks ! ========\\n\")     for i, chunk in enumerate(chunks):         print(f\"--------------------- chunk {i} ---------------------\")         print(chunk.get_text()) In\u00a0[8]: Copied! <pre>from chunknorris.parsers import MarkdownParser # &lt;- you can use any parser you want as long as the are compatible with MarkdownChunker\nfrom chunknorris.chunkers import MarkdownChunker # &lt;- tutorial is essentially about this guy\nfrom chunknorris.pipelines import BasePipeline\nfrom IPython.display import Markdown\n</pre> from chunknorris.parsers import MarkdownParser # &lt;- you can use any parser you want as long as the are compatible with MarkdownChunker from chunknorris.chunkers import MarkdownChunker # &lt;- tutorial is essentially about this guy from chunknorris.pipelines import BasePipeline from IPython.display import Markdown <p>For this tutorial we will consider this easy Markdown :</p> In\u00a0[9]: Copied! <pre>md_string = \"\"\"\n# This is header 1\n\nThis is some intruction text after header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n\"\"\"\nMarkdown(md_string)\n</pre> md_string = \"\"\" # This is header 1  This is some intruction text after header 1  ## This is SUBheader 1  This is some intruction text after header 1  ### This is an h3 header  This is the content of the h3 subsection  ### This is ANOTHER h3 header  This is the other content of the h3 subsection \"\"\" Markdown(md_string) Out[9]: In\u00a0[10]: Copied! <pre># Pipeline with default parameters:\npipeline = BasePipeline(MarkdownParser(), MarkdownChunker())\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> # Pipeline with default parameters: pipeline = BasePipeline(MarkdownParser(), MarkdownChunker()) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0001 seconds\n</pre> <pre>\n======= Got 1 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\nThis is some intruction text after header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> In\u00a0[11]: Copied! <pre>chunker = MarkdownChunker(\n    max_chunk_word_count=50, \n    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n    )\npipeline = BasePipeline(MarkdownParser(), chunker)\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> chunker = MarkdownChunker(     max_chunk_word_count=50,      min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default     ) pipeline = BasePipeline(MarkdownParser(), chunker) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0002 seconds\n</pre> <pre>\n======= Got 2 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\nThis is some intruction text after header 1\n--------------------- chunk 1 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> <p>As we can see, with parameter <code>max_chunk_word_count=50</code> the chunk is introduction part is split from the rest to make sure both chunks are below 50 words. We could even decrease that number to split the second chunk even more.</p> <p>\ud83d\udca1 Pro tip : Want to make sure all the headers are used to build chunks ? Just set <code>max_chunk_word_count=0</code> and the chunker will try to make chunks of 0 word count, hence using all headers available.</p> In\u00a0[12]: Copied! <pre>chunker = MarkdownChunker(\n    max_chunk_word_count=0, \n    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n    )\npipeline = BasePipeline(MarkdownParser(), chunker)\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> chunker = MarkdownChunker(     max_chunk_word_count=0,      min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default     ) pipeline = BasePipeline(MarkdownParser(), chunker) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0002 seconds\n</pre> <pre>\n======= Got 4 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\nThis is some intruction text after header 1\n--------------------- chunk 1 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n--------------------- chunk 2 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n--------------------- chunk 3 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> In\u00a0[13]: Copied! <pre>chunker = MarkdownChunker(\n    max_chunk_word_count=0,\n    max_headers_to_use=\"h2\", # &lt;- only use h1 and h2 to split chunks\n    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n    )\npipeline = BasePipeline(MarkdownParser(), chunker)\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> chunker = MarkdownChunker(     max_chunk_word_count=0,     max_headers_to_use=\"h2\", # &lt;- only use h1 and h2 to split chunks     min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default     ) pipeline = BasePipeline(MarkdownParser(), chunker) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0001 seconds\n</pre> <pre>\n======= Got 2 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\nThis is some intruction text after header 1\n--------------------- chunk 1 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> <p>Now we only have 2 chunks, as h3 headers were not allowed to be used to split the chunks.</p> In\u00a0[14]: Copied! <pre>chunker = MarkdownChunker(\n    max_chunk_word_count=0,\n    max_headers_to_use=\"h2\", # &lt;- only use h1 and h2 to split chunks\n    hard_max_chunk_word_count=20,\n    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n    )\npipeline = BasePipeline(MarkdownParser(), chunker)\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> chunker = MarkdownChunker(     max_chunk_word_count=0,     max_headers_to_use=\"h2\", # &lt;- only use h1 and h2 to split chunks     hard_max_chunk_word_count=20,     min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default     ) pipeline = BasePipeline(MarkdownParser(), chunker) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0002 seconds\n</pre> <pre>\n======= Got 3 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\nThis is some intruction text after header 1\n--------------------- chunk 1 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n--------------------- chunk 2 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> <p>There we go, the second chunk from before has been splitted in 2 !</p> In\u00a0[17]: Copied! <pre>import tiktoken\n\nchunker = MarkdownChunker(\n    min_chunk_word_count=0, # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n    hard_max_chunk_token_count=20,\n    tokenizer=tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n    )\npipeline = BasePipeline(MarkdownParser(), chunker)\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> import tiktoken  chunker = MarkdownChunker(     min_chunk_word_count=0, # we set this to 0 because the chunker automatically discard chunks below 15 words by default     hard_max_chunk_token_count=20,     tokenizer=tiktoken.encoding_for_model(\"text-embedding-3-small\"),     ) pipeline = BasePipeline(MarkdownParser(), chunker) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2025-05-15 09:10:ChunkNorris:INFO:Function \"chunk\" took 0.0005 seconds\n</pre> <pre>\n======= Got 4 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\nThis is some intruction text after header 1\n--------------------- chunk 1 ---------------------\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n--------------------- chunk 2 ---------------------\n### This is an h3 header\n\nThis is the content of the h3 subsection\n--------------------- chunk 3 ---------------------\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> In\u00a0[34]: Copied! <pre>chunker = MarkdownChunker(\n    max_chunk_word_count=0,\n    max_headers_to_use=\"h2\",\n    hard_max_chunk_word_count=20,\n    min_chunk_word_count=10 # Discard chunks will less than 10 words (excluding headers)\n    )\npipeline = BasePipeline(MarkdownParser(), chunker)\nchunks = pipeline.chunk_string(md_string)\nprint_chunking_result(chunks)\n</pre> chunker = MarkdownChunker(     max_chunk_word_count=0,     max_headers_to_use=\"h2\",     hard_max_chunk_word_count=20,     min_chunk_word_count=10 # Discard chunks will less than 10 words (excluding headers)     ) pipeline = BasePipeline(MarkdownParser(), chunker) chunks = pipeline.chunk_string(md_string) print_chunking_result(chunks) <pre>2024-12-17 10:59:ChunkNorris:INFO:Function \"chunk\" took 0.0003 seconds\n</pre> <pre>\n======= Got 2 chunks ! ========\n\n--------------------- chunk 0 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\nThis is some intruction text after header 1\n\n### This is an h3 header\n\nThis is the content of the h3 subsection\n--------------------- chunk 1 ---------------------\n# This is header 1\n\n## This is SUBheader 1\n\n### This is ANOTHER h3 header\n\nThis is the other content of the h3 subsection\n</pre> <p>There you go. The first chunk from before has been dicarded.</p> In\u00a0[9]: Copied! <pre># Get the MarkdownDoc that can be fed to the chunker\nparser = MarkdownParser()\nmd_doc = parser.parse_string(md_string)\n\n# New, get the TocTree\nchunker = MarkdownChunker()\ntoc_tree = chunker.get_toc_tree(md_doc.content)\n\n# Save the TocTree to have a look at it\ntoc_tree.to_json(output_path=\"toc_tree.json\")\n</pre> # Get the MarkdownDoc that can be fed to the chunker parser = MarkdownParser() md_doc = parser.parse_string(md_string)  # New, get the TocTree chunker = MarkdownChunker() toc_tree = chunker.get_toc_tree(md_doc.content)  # Save the TocTree to have a look at it toc_tree.to_json(output_path=\"toc_tree.json\")"},{"location":"examples/advanced_chunking/#influence-chunking-behavior","title":"Influence chunking behavior\u00b6","text":"<p>One may want to influence how the chunks are built by passing parameters to the <code>MarkdownChunker</code>. This notebook intends to git a feeling of \"which parameter does what\". Happy chunking ! \ud83d\udd2a</p>"},{"location":"examples/advanced_chunking/#this-is-header-1","title":"This is header 1\u00b6","text":"<p>This is some intruction text after header 1</p>"},{"location":"examples/advanced_chunking/#this-is-subheader-1","title":"This is SUBheader 1\u00b6","text":"<p>This is some intruction text after header 1</p>"},{"location":"examples/advanced_chunking/#this-is-an-h3-header","title":"This is an h3 header\u00b6","text":"<p>This is the content of the h3 subsection</p>"},{"location":"examples/advanced_chunking/#this-is-another-h3-header","title":"This is ANOTHER h3 header\u00b6","text":"<p>This is the other content of the h3 subsection</p>"},{"location":"examples/advanced_chunking/#impact-of-each-argument","title":"Impact of each argument\u00b6","text":""},{"location":"examples/advanced_chunking/#max_chunk_word_count","title":"max_chunk_word_count\u00b6","text":"<p>We can see that we got only one chunk, despite the presence of headers !</p> <p>This is due to <code>MarkdownChunker</code>'s parameter <code>max_chunk_word_count</code>. The default value is <code>200</code>, meaning that the chunker will try to make chunks of approximately 200 words. I a chunk is bigger that this, only then it will be chunked using its header.</p> <p>This may sound weird, but embedding models are still sensitive to the length of the text. Consequently, \"I have a dog\" may be more similar to \"I love electronic music\" than a whole paragraph about dogs. \ud83d\udca1 By ensuring the resulting chunks are of similar sizes, we minimize the influence of the chunk's size in the embedding and make it more about the chunk's meaning.</p> <p>Let's play with <code>max_chunk_word_count</code> a bit :</p>"},{"location":"examples/advanced_chunking/#max_headers_to_use","title":"max_headers_to_use\u00b6","text":"<p>You may want to get chunks as small as possible, but avoid using headers level that are too low. Indeed, it is common that list-items in html are h5 headers and you wouldn't want each item in the list ot be a chunk.</p> <p>By default, MarkdownChunker will only use headers up to <code>H4</code>, and won't use h5 and h6. But let's change this and see how it affects behavior.</p>"},{"location":"examples/advanced_chunking/#hard_max_chunk_word_count","title":"hard_max_chunk_word_count\u00b6","text":"<p>Now, we saw that forbidding the <code>MarkdownChunker</code> to use headers will enforce chunks to be bigger that that what is requested by the <code>max_chunk_word_count</code> parameter. The same happens if no header is available in the markdown : the chunker will try to make chunks of requested size, but if no header is available it will leave the chunk \"as is\".</p> <p>This may lead to veeeery big chunks and we don't want that (and most embedding API will trigger an error if your chunk is bigger than the model's context window).</p> <p>That's when the <code>hard_max_chunk_word_count</code> comes into play. This parameter allows you to set a kind of hard limit for the chunk. Chunks bigger that the limit will be splitted to fit the limit. Why kind of ? Because <code>MarkdownChunker</code> will avoid splitting in the middle of a code block, or in a table so you may still have resulting chunks that are slightly bigger that this.</p>"},{"location":"examples/advanced_chunking/#hard_max_chunk_token_count-and-tokenizer","title":"hard_max_chunk_token_count and tokenizer\u00b6","text":"<p>This parameter allow to set an actual hard limit in terms of token to avoid any errors regarding API calls to embedding model providers.</p> <p>When <code>hard_max_chunk_token_count</code> is set to an int value, the provided <code>tokenizer</code> will be used to count tokens. Chunks bigger than the value specified will be split into subchunks, trying to equilibrate their size and considering newlines to avoid random cuts.</p> <p>The provided tokenizer MUST have an <code>encode(str) -&gt; list[int]</code> method, that takes a string as input, and returns a list of tokens. For example, the tiktoken package nativiely have such method.</p>"},{"location":"examples/advanced_chunking/#min_chunk_word_count","title":"min_chunk_word_count\u00b6","text":"<p>When all this chunking is happening, you may have small chunks remaining. This is frequent for webscrapping for example, where some pages will be empty or just have a title.</p> <p>As said before, these small chunk can be a pain because in the context of information retrieval based on queries thay tend to come up because of their similar size with the queries.</p> <p>To avoid having these small chunks lost in a database full of great chunks, the <code>min_chunk_word_count</code> argument is here. This will allow chunks will less words than the limit to be automatically discarded. The default value is <code>15</code> but you may to set it to 0 if you absolutely wish to keep every chunks.</p>"},{"location":"examples/advanced_chunking/#work-with-the-toc-tree","title":"Work with the TOC tree\u00b6","text":"<p>To build the chunks according to the markdown headers, the MarkdownChunker uses a <code>TocTree</code> object. The <code>TocTree</code> represents the table of content, and the content of each part.</p> <p>Whether it is for debugging, or because you want to implement some  custom chunking strategy, you may want to have a look at the table of content that has been parsed from your document.</p>"},{"location":"examples/advanced_chunking/#conclusion","title":"Conclusion\u00b6","text":"<p>\ud83e\uddea Feel free to experiments with these parameters to get the chunks that suit your data.</p> <p>If this still seem a bit obscure, don't worry the default parameters have been tested on multiple custom dataset and have proven to work well \ud83d\ude09 !</p>"},{"location":"examples/custom_parser/","title":"Implement a custom Parser (NotebookParser)","text":"In\u00a0[\u00a0]: Copied! <pre># Import components\nfrom typing import Any\nimport json\nfrom IPython.display import Markdown\nfrom chunknorris.parsers import AbstractParser # &lt;-- our custom parser must inherit from this\nfrom chunknorris.core.components import MarkdownDoc # &lt;-- object ot be fed in chunker\n</pre> # Import components from typing import Any import json from IPython.display import Markdown from chunknorris.parsers import AbstractParser # &lt;-- our custom parser must inherit from this from chunknorris.core.components import MarkdownDoc # &lt;-- object ot be fed in chunker In\u00a0[2]: Copied! <pre># Base of our class\nclass NotebookParser(AbstractParser): # inherit from abstract parser\n    def parse_file(self, filepath: str) -&gt; MarkdownDoc:\n        pass\n\n    def parse_string(self, string: str) -&gt; MarkdownDoc:\n        pass # We have to fill this\n</pre> # Base of our class class NotebookParser(AbstractParser): # inherit from abstract parser     def parse_file(self, filepath: str) -&gt; MarkdownDoc:         pass      def parse_string(self, string: str) -&gt; MarkdownDoc:         pass # We have to fill this In\u00a0[3]: Copied! <pre>class NotebookParser(AbstractParser): # inherit from abstract parser\n    def __init__(self, include_code_cells_outputs: bool = False) -&gt; None:\n        self.include_code_cells_outputs = include_code_cells_outputs\n\n    def parse_file(self, filepath: str) -&gt; MarkdownDoc:\n        \"\"\"chunks a notebook .ipynb file\"\"\"\n        file_content = self.read_file(filepath)\n        md_string = self.parse_notebook_content(file_content)\n        \n        return MarkdownDoc.from_string(md_string) # we don't return directly the markdown string, but build a MarkdownDoc with\n\n    def parse_string(self, string: str) -&gt; MarkdownDoc:\n        raise NotImplementedError # We won't implement this as it is unlikely that the notebook content fill be passed as a string.\n\n    @staticmethod\n    def read_file(filepath: str) -&gt; dict[str, Any]:\n        \"\"\"Reads a .ipynb file and returns its \n        content as a json dict.\n\n        Args:\n            filepath (str): path to the file\n\n        Returns:\n            dict[str, Any]: the json content of the ipynb file\n        \"\"\"\n        if not filepath.endswith(\".ipynb\"):\n            raise ValueError(\"Only .ipynb files can be passed to NotebookParser.\")\n        with open(filepath, \"r\", encoding=\"utf8\") as file:\n            content = json.load(file)\n\n        return content\n    \n    def parse_notebook_content(self, notebook_content: dict[str, Any]) -&gt; str:\n        \"\"\"Parses\n\n        Args:\n            notebook_content (dict[str, Any]): the content of the notebook, as a json file.\n                It should be a dict of structure:\n                {'cells': [{\n                    'cell_type': 'markdown',\n                    'metadata': {},\n                    'source': &lt;list of lines&gt;\n                    }...]\n\n        Returns:\n            str: the markdown string parsed from the notebook content\n        \"\"\"\n        kernel_language = notebook_content[\"metadata\"][\"kernelspec\"][\"language\"]\\\n            if notebook_content[\"metadata\"] else \"\"\n        md_string = \"\"\n        for cell in notebook_content[\"cells\"]:\n            match cell[\"cell_type\"]:\n                case \"markdown\" | \"raw\":\n                    md_string += \"\".join(cell[\"source\"]) + \"\\n\\n\"\n                case \"code\":\n                    language = cell[\"metadata\"][\"kernelspec\"][\"language\"] if cell[\"metadata\"] else kernel_language\n                    md_string += \"```\" + language + \"\\n\" + \"\".join(cell['source']) + \"\\n```\\n\\n\"\n                    if self.include_code_cells_outputs:\n                        md_string += \"\".join(cell[\"outputs\"].get(\"data\", {}).get('text/plain', \"\")) + \"\\n\\n\"\n                case _:\n                    pass\n\n        return md_string\n</pre> class NotebookParser(AbstractParser): # inherit from abstract parser     def __init__(self, include_code_cells_outputs: bool = False) -&gt; None:         self.include_code_cells_outputs = include_code_cells_outputs      def parse_file(self, filepath: str) -&gt; MarkdownDoc:         \"\"\"chunks a notebook .ipynb file\"\"\"         file_content = self.read_file(filepath)         md_string = self.parse_notebook_content(file_content)                  return MarkdownDoc.from_string(md_string) # we don't return directly the markdown string, but build a MarkdownDoc with      def parse_string(self, string: str) -&gt; MarkdownDoc:         raise NotImplementedError # We won't implement this as it is unlikely that the notebook content fill be passed as a string.      @staticmethod     def read_file(filepath: str) -&gt; dict[str, Any]:         \"\"\"Reads a .ipynb file and returns its          content as a json dict.          Args:             filepath (str): path to the file          Returns:             dict[str, Any]: the json content of the ipynb file         \"\"\"         if not filepath.endswith(\".ipynb\"):             raise ValueError(\"Only .ipynb files can be passed to NotebookParser.\")         with open(filepath, \"r\", encoding=\"utf8\") as file:             content = json.load(file)          return content          def parse_notebook_content(self, notebook_content: dict[str, Any]) -&gt; str:         \"\"\"Parses          Args:             notebook_content (dict[str, Any]): the content of the notebook, as a json file.                 It should be a dict of structure:                 {'cells': [{                     'cell_type': 'markdown',                     'metadata': {},                     'source':                      }...]          Returns:             str: the markdown string parsed from the notebook content         \"\"\"         kernel_language = notebook_content[\"metadata\"][\"kernelspec\"][\"language\"]\\             if notebook_content[\"metadata\"] else \"\"         md_string = \"\"         for cell in notebook_content[\"cells\"]:             match cell[\"cell_type\"]:                 case \"markdown\" | \"raw\":                     md_string += \"\".join(cell[\"source\"]) + \"\\n\\n\"                 case \"code\":                     language = cell[\"metadata\"][\"kernelspec\"][\"language\"] if cell[\"metadata\"] else kernel_language                     md_string += \"```\" + language + \"\\n\" + \"\".join(cell['source']) + \"\\n```\\n\\n\"                     if self.include_code_cells_outputs:                         md_string += \"\".join(cell[\"outputs\"].get(\"data\", {}).get('text/plain', \"\")) + \"\\n\\n\"                 case _:                     pass          return md_string In\u00a0[4]: Copied! <pre>path_to_notebook = \"./custom_parser.ipynb\" # as an example we will use... this notebook !\nnotebook_parser = NotebookParser(include_code_cells_outputs=False)\nmd_doc = notebook_parser.parse_file(path_to_notebook)\n</pre> path_to_notebook = \"./custom_parser.ipynb\" # as an example we will use... this notebook ! notebook_parser = NotebookParser(include_code_cells_outputs=False) md_doc = notebook_parser.parse_file(path_to_notebook) In\u00a0[5]: Copied! <pre># Before feeding the parsed result to the chunker, **let's have a look** at the markdown it outputs.\nMarkdown(md_doc.to_string()[:1400] + \" [...]\") # only print out the first 1400 caracters\n</pre> # Before feeding the parsed result to the chunker, **let's have a look** at the markdown it outputs. Markdown(md_doc.to_string()[:1400] + \" [...]\") # only print out the first 1400 caracters Out[5]: <p>That parsed result looks great ! Now let's chunk it !</p> <p>You can directly feed the <code>MarkdownDoc</code> to the <code>MarkdownChunker.chunk()</code> method. But I would suggest to use the BasePipeline to do this as it enables extra functionnality for saving the chunks.</p> In\u00a0[6]: Copied! <pre>from chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n</pre> from chunknorris.chunkers import MarkdownChunker from chunknorris.pipelines import BasePipeline In\u00a0[7]: Copied! <pre>pipe = BasePipeline(\n    parser=NotebookParser(), \n    chunker=MarkdownChunker(max_chunk_word_count=100)\n    )\n\nchunks = pipe.chunk_file(path_to_notebook)\nprint(f\"Got {len(chunks)} chunks !\")\nfor i, chunk in enumerate(chunks[:3]):\n    print(f\"============ chunk {i} ============\")\n    print(chunk)\n</pre> pipe = BasePipeline(     parser=NotebookParser(),      chunker=MarkdownChunker(max_chunk_word_count=100)     )  chunks = pipe.chunk_file(path_to_notebook) print(f\"Got {len(chunks)} chunks !\") for i, chunk in enumerate(chunks[:3]):     print(f\"============ chunk {i} ============\")     print(chunk) <pre>2024-12-20 10:11:ChunkNorris:INFO:Function \"chunk\" took 0.0014 seconds\n</pre> <pre>Got 6 chunks !\n============ chunk 0 ============\n# Implement a custom Parser (NotebookParser)\n\nThis tutorial is designed to provide you with additional tools for utilizing ``chunknorris`` in your specific applications. All components, including the Parser, Chunker, and Pipelines, can be tailored to meet your requirements.\n\nIn this tutorial, we will focus on how to implement a **custom parser**.\n\n\u26a0\ufe0f **Important note**: Following the implementation of this tutorial, the ``JupyterNotebookParser`` has been implemented. It's a more robust implementation than what's presented here, so **if your aim is to parse jupyter notebooks, it's advisable to use the ``JupyterNotebookParser``**.\n============ chunk 1 ============\n# Implement a custom Parser (NotebookParser)\n\n## Goal\n\nIn this tutorial let's consider you want to implement a **custom Notebook parser**.\n\nAs we still want to leverage the ability of ``chunknorris`` to chunk efficiently, we must implement a parser that can be plugged into the ``MarkdownChunker`` through a pipeline. The ``MarkdownChunker`` takes as input a ``MarkdownDoc`` object, our parser has to output the markdown content in that format.\n\n```python\n# Import components\nfrom typing import Any\nimport json\nfrom IPython.display import Markdown\nfrom chunknorris.parsers import AbstractParser # &lt;-- our custom parser must inherit from this\nfrom chunknorris.parsers.markdown.components import MarkdownDoc # &lt;-- object ot be fed in chunker\n```\n============ chunk 2 ============\n# Implement a custom Parser (NotebookParser)\n\n## Starting point\n\nWe start by importing the ``AbstractParser``. Every parser in chunknorris must inherit from it. This class only need you to implement two method, which will enable your parser to fit well with the ``chunknorris``' pipelines :\n\n- chunk_string(string : str) to parse a string.\n- chunk_file(filepath : str) to parse a file given a filepath.\n\nBoth must return a ``MarkdownDoc`` object.\n\n```python\n# Base of our class\nclass NotebookParser(AbstractParser): # inherit from abstract parser\ndef parse_file(self, filepath: str) -&gt; MarkdownDoc:\npass\n\ndef parse_string(self, string: str) -&gt; MarkdownDoc:\npass # We have to fill this\n```\n</pre>"},{"location":"examples/custom_parser/#implement-a-custom-parser-notebookparser","title":"Implement a custom Parser (NotebookParser)\u00b6","text":"<p>This tutorial is designed to provide you with additional tools for utilizing <code>chunknorris</code> in your specific applications. All components, including the Parser, Chunker, and Pipelines, can be tailored to meet your requirements.</p> <p>In this tutorial, we will focus on how to implement a custom parser.</p> <p>\u26a0\ufe0f Important note: Following the implementation of this tutorial, the <code>JupyterNotebookParser</code> has been implemented. It's a more robust implementation than what's presented here, so if your aim is to parse jupyter notebooks, it's advisable to use the <code>JupyterNotebookParser</code>.</p>"},{"location":"examples/custom_parser/#goal","title":"Goal\u00b6","text":"<p>In this tutorial let's consider you want to implement a custom Notebook parser.</p> <p>As we still want to leverage the ability of <code>chunknorris</code> to chunk efficiently, we must implement a parser that can be plugged into the <code>MarkdownChunker</code> through a pipeline. The <code>MarkdownChunker</code> takes as input a <code>MarkdownDoc</code> object, our parser has to output the markdown content in that format.</p>"},{"location":"examples/custom_parser/#starting-point","title":"Starting point\u00b6","text":"<p>We start by importing the <code>AbstractParser</code>. Every parser in chunknorris must inherit from it. This class only need you to implement two method, which will enable your parser to fit well with the <code>chunknorris</code>' pipelines :</p> <ul> <li>chunk_string(string : str) to parse a string.</li> <li>chunk_file(filepath : str) to parse a file given a filepath.</li> </ul> <p>Both must return a <code>MarkdownDoc</code> object.</p>"},{"location":"examples/custom_parser/#add-functionnality","title":"Add functionnality\u00b6","text":"<p>Let's add some functionnality to read and parse the file !</p> <p>We will implement 2 methods :</p> <ul> <li><code>read_file()</code> to read the file</li> <li><code>parse_notebook_content()</code> that parses the \"markdown\" and \"code\" cells of the notebook.</li> </ul> <p>Much more parsing work could be done but we will limite to this for the tutorial. Let's have a look at our <code>NotebookParser</code> class now:</p>"},{"location":"examples/custom_parser/#use-our-parser-to-get-chunks","title":"Use our parser to get chunks\u00b6","text":"<p>Now the parser is ready, let's use it !</p>"},{"location":"examples/custom_parser/#implement-a-custom-parser-notebookparser","title":"Implement a custom Parser (NotebookParser)\u00b6","text":"<p>This tutorial is designed to provide you with additional tools for utilizing <code>chunknorris</code> in your specific applications. All components, including the Parser, Chunker, and Pipelines, can be tailored to meet your requirements.</p> <p>In this tutorial, we will focus on how to implement a custom parser.</p> <p>\u26a0\ufe0f Important note: Following the implementation of this tutorial, the <code>JupyterNotebookParser</code> has been implemented. It's a more robust implementation than what's presented here, so if your aim is to parse jupyter notebooks, it's advisable to use the <code>JupyterNotebookParser</code>.</p>"},{"location":"examples/custom_parser/#goal","title":"Goal\u00b6","text":"<p>In this tutorial let's consider you want to implement a custom Notebook parser.</p> <p>As we still want to leverage the ability of <code>chunknorris</code> to chunk efficiently, we must implement a parser that can be plugged into the <code>MarkdownChunker</code> through a pipeline. The <code>MarkdownChunker</code> takes as input a <code>MarkdownDoc</code> object, our parser has to output the markdown content in that format.</p> <pre># Import components\nfrom typing import Any\nimport json\nfrom IPython.display import Markdown\nfrom chunknorris.parsers import AbstractParser # &lt;-- our custom parser must inherit from this\nfrom chunknorris.parsers.markdown.components import MarkdownDoc # &lt;-- object ot be fed in chunker\n</pre>"},{"location":"examples/custom_parser/#starting-point","title":"Starting point\u00b6","text":"<p>We start by importing the <code>AbstractParser</code> [...]</p>"},{"location":"examples/custom_parser/#conclusion","title":"Conclusion\u00b6","text":"<p>There you go ! Take note the <code>chunknorris</code> will always try to preserve the intergrity of code blocks.</p> <p>One last tip: if you wish to customize the behavior of one specific parser instead (HTMLParser for example), you might want to inherit directly from that parser instead of starting from stratch with AbstractParser.</p>"},{"location":"examples/docx_chunking/","title":"HTML file chunking","text":"In\u00a0[2]: Copied! <pre># If needed, install chunknorris\n%pip install chunknorris -q\n</pre> # If needed, install chunknorris %pip install chunknorris -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[3]: Copied! <pre># imported the required chunknorris components\nfrom chunknorris.parsers import DocxParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n</pre> # imported the required chunknorris components from chunknorris.parsers import DocxParser from chunknorris.chunkers import MarkdownChunker from chunknorris.pipelines import BasePipeline <p>3 components are needed to chunk a markdown file:</p> <ul> <li><p><code>DocxParser</code>: Behind the scene, it will use <code>mammoth</code> and <code>markfownify</code> to convert the docx file to markdown, and perform extra cleaning of the content. It returns a <code>MarkdownDoc</code>.</p> </li> <li><p><code>MarkdownChunker</code>: the chunker takes as input a <code>MarkdownDoc</code> and performs chunking in multiple <code>Chunks</code> objects.</p> </li> <li><p><code>BasePipeline</code>: this pipline just plugs together the parser and the chunker so that the output of the parser in fed to the chunker.</p> </li> </ul> In\u00a0[5]: Copied! <pre># instanciate\npipeline = BasePipeline(DocxParser(), MarkdownChunker())\n</pre> # instanciate pipeline = BasePipeline(DocxParser(), MarkdownChunker()) In\u00a0[7]: Copied! <pre># Get those chunks !\npath_to_file = \"../../tests/test_files/file.docx\"\nchunks = pipeline.chunk_file(path_to_file)\nprint(f\"Got {len(chunks)} chunks !\")\n</pre> # Get those chunks ! path_to_file = \"../../tests/test_files/file.docx\" chunks = pipeline.chunk_file(path_to_file) print(f\"Got {len(chunks)} chunks !\") <pre>2025-02-21 15:17:ChunkNorris:INFO:Function \"chunk\" took 0.0014 seconds\n</pre> <pre>Got 11 chunks !\n</pre> In\u00a0[9]: Copied! <pre># Let's look at the chunks\nfor i, chunk in enumerate(chunks[1:4]): # we only look at the 2 first chunks\n    print(f\"\\n------------- chunk {i} ----------------\\n\")\n    print(chunk.get_text())\n</pre> # Let's look at the chunks for i, chunk in enumerate(chunks[1:4]): # we only look at the 2 first chunks     print(f\"\\n------------- chunk {i} ----------------\\n\")     print(chunk.get_text()) <pre>\n------------- chunk 0 ----------------\n\n# Dummy Table\n\n| | Age | Likes Pdf | Likes AI |\n| --- | --- | --- | --- |\n| Marc | 20 | Yes ? | **Yes !** |\n| Alice | 30 | **No** |\n| Rob | 40 |\n| Julia | 50 |\n| The cat | 60 | **No** |\n\nThis is a dummy table that has nothing to do with the rest of the content but is here for testing purposes\n\n------------- chunk 1 ----------------\n\n# Inse\u0301rer une table des matie\u0300res\n\nPour ajouter une table des matie\u0300res, de\u0301cidez simplement de l\u2019emplacement souhaite\u0301. Word se charge du reste.\n\nEssayez par vous-me\u0302me : appuyez sur ENTRE\u0301E apre\u0300s le premier paragraphe dans ce document pour obtenir une nouvelle ligne. Acce\u0301dez ensuite a\u0300 l\u2019onglet **Re\u0301fe\u0301rences**, se\u0301lectionnez **Table des matie\u0300res** et choisissez une table des matie\u0300res dans la liste.\n\nVous avez termine\u0301 ! Word a de\u0301tecte\u0301 tous les titres dans ce document et ajoute\u0301 une table des matie\u0300res.\n\n------------- chunk 2 ----------------\n\n# Mise a\u0300 jour quand il y a des changements\n\nLe travail ne s\u2019arre\u0302te avec simplement la cre\u0301ation d\u2019une table des matie\u0300res. Word est suffisamment intelligent pour effectuer le suivi des e\u0301le\u0301ments du document pour vous e\u0301viter ce soucis. Lorsque les choses changent, mettez simplement a\u0300 jour la table des matie\u0300res.\n\nEssayez par vous-me\u0302me : Mettre a\u0300 jour votre table des matie\u0300res.\n\n1. Placez le curseur apre\u0300s le paragraphe qui se termine par, \u00ab lorsque les choses changent, mettez simplement a\u0300 jour la table des matie\u0300res \u00bb (ci-dessus), et appuyez sur Ctrl + Entre\u0301e pour \u00ab pousser \u00bb cette section sur la page 3.\n2. Acce\u0301dez a\u0300 votre table des matie\u0300res et cliquez n\u2019importe ou\u0300 dedans. Puis cliquez sur **Mettre a\u0300 jour la table des matie\u0300res**, puis cliquez sur **OK** (**Mettre a\u0300 jour les nume\u0301ros de page uniquement** est se\u0301lectionne\u0301 par de\u0301faut).\n\nWord a mis a\u0300 jour la ligne Mettre a\u0300 jour quand il y a des changements de page 2 a\u0300 page 3.\n\nUtiliser des styles pour les en-te\u0302tes\n\nLa magie des tables des matie\u0300res re\u0301side dans les styles qui servent a\u0300 mettre en forme les titres. L\u2019en-te\u0302te de cette section Utiliser des styles pour les en-te\u0302tes, peut ressembler a\u0300 un titre, mais il ne fonctionne pas comme un titre. Il a e\u0301te\u0301 mis en forme morceau par morceau (taille de police, soulignement) au lieu d\u2019e\u0302tre mis en forme avec un style. Vous notez comment il ne se trouve pas dans la table des matie\u0300res que vous avez ajoute\u0301e ? Pour ajouter un titre a\u0300 une table des matie\u0300res, il doit e\u0302tre mis en forme avec le style Titre 1.\n\nEssayez par vous-me\u0302me : Mettez a\u0300 jour le style, puis mettez a\u0300 jour la table des matie\u0300res.\n\n1. Cliquez dans le titre ci-dessus (Utiliser des styles pour les en-te\u0302tes), veillez a\u0300 cliquer simplement, ne se\u0301lectionnez aucun e\u0301le\u0301ment.\n2. Sous l\u2019onglet **Accueil**, recherchez **Styles**, puis se\u0301lectionnez **Titre 1** (raccourci clavier Ctrl+Alt+1) : Alt+Ctrl+1).\n3. Mettez a\u0300 jour votre table des matie\u0300res comme vous l\u2019avez fait pre\u0301ce\u0301demment, mais cette fois, se\u0301lectionnez **Mettre a\u0300 jour toute la table** (au lieu de **Mettre a\u0300 jour les nume\u0301ros de page uniquement**) car il n\u2019y a pas que les nume\u0301ros de page qui ont change\u0301.\n\nA\u0300 pre\u0301sent, Word sait que ce paragraphe est un titre et l\u2019inclut dans la table des matie\u0300res.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Let's save the chunks. We can just pass the chunks we obtain and the filename we want\npipeline.save_chunks(chunks, \"mychunk.json\")\n</pre> # Let's save the chunks. We can just pass the chunks we obtain and the filename we want pipeline.save_chunks(chunks, \"mychunk.json\")"},{"location":"examples/docx_chunking/#html-file-chunking","title":"HTML file chunking\u00b6","text":"<p>This notebook aims at showing a simple example of chunking for Microsoft Word documents (.docx).</p>"},{"location":"examples/docx_chunking/#pipeline-setup","title":"Pipeline setup\u00b6","text":""},{"location":"examples/docx_chunking/#view-the-chunks","title":"View the chunks\u00b6","text":""},{"location":"examples/docx_chunking/#save-the-chunks","title":"Save the chunks\u00b6","text":"<p>The pipeline as a method to save the chunks and their attributes as a json file. Here is how to use it.</p>"},{"location":"examples/html_chunking/","title":"HTML file chunking","text":"In\u00a0[2]: Copied! <pre># If needed, install chunknorris\n%pip install chunknorris -q\n</pre> # If needed, install chunknorris %pip install chunknorris -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[4]: Copied! <pre># imported the required chunknorris components\nfrom chunknorris.parsers import HTMLParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n</pre> # imported the required chunknorris components from chunknorris.parsers import HTMLParser from chunknorris.chunkers import MarkdownChunker from chunknorris.pipelines import BasePipeline <p>3 components are needed to chunk a HTML file:</p> <ul> <li><p><code>HTMLParser</code>: Behind the scene, it will use <code>markdownify</code> to convert the html to markdown, and perform extra cleaning of the content. It returns a <code>MarkdownDoc</code>.</p> </li> <li><p><code>MarkdownChunker</code>: the chunker takes as input a <code>MarkdownDoc</code> and performs chunking in multiple <code>Chunks</code> objects.</p> </li> <li><p><code>BasePipeline</code>: this pipline just plugs together the parser and the chunker so that the output of the parser in fed to the chunker.</p> </li> </ul> In\u00a0[5]: Copied! <pre># instanciate\npipeline = BasePipeline(HTMLParser(), MarkdownChunker())\n</pre> # instanciate pipeline = BasePipeline(HTMLParser(), MarkdownChunker()) In\u00a0[6]: Copied! <pre># Get those chunks !\npath_to_html_file = \"../../tests/test_files/file.html\"\nchunks = pipeline.chunk_file(path_to_html_file)\nprint(f\"Got {len(chunks)} chunks !\")\n</pre> # Get those chunks ! path_to_html_file = \"../../tests/test_files/file.html\" chunks = pipeline.chunk_file(path_to_html_file) print(f\"Got {len(chunks)} chunks !\") <pre>2024-12-17 17:33:ChunkNorris:INFO:Function \"chunk\" took 0.0032 seconds\n</pre> <pre>Got 17 chunks !\n</pre> In\u00a0[8]: Copied! <pre># Let's look at the chunks\nfor i, chunk in enumerate(chunks[:2]): # we only look at the 2 first chunks\n    print(f\"\\n------------- chunk {i} ----------------\\n\")\n    print(chunk.get_text())\n</pre> # Let's look at the chunks for i, chunk in enumerate(chunks[:2]): # we only look at the 2 first chunks     print(f\"\\n------------- chunk {i} ----------------\\n\")     print(chunk.get_text()) <pre>\n------------- chunk 0 ----------------\n\n# Le Jardin\n\nUn [jardin japonais](https://fr.wikipedia.org/wiki/Jardin_japonais \"Jardin japonais\").\n\nUne femme de 87 ans en train de cultiver son jardin. [Comte\u0301 de Harju](https://fr.wikipedia.org/wiki/Comt%C3%A9_de_Harju \"Comte\u0301 de Harju\"), Estonie, juin 2016.\n\nUn **jardin** est un lieu durablement et hypothe\u0301tiquement ame\u0301nage\u0301 ou\u0300 l'on cultive\nde fac\u0327on ordonne\u0301e des [plantes](https://fr.wikipedia.org/wiki/Plante \"Plante\") domestique\u0301es ou\nse\u0301lectionne\u0301es. Il est le produit de\nla technique du [jardinage](https://fr.wikipedia.org/wiki/Jardinage \"Jardinage\") et, comme elle, il\nremonte au moins a\u0300 l'Antiquite\u0301. Les diffe\u0301rentes cultures humaines dans le monde, au\nfil des e\u0301poques,\nont invente\u0301 de nombreux types et styles de jardins. Lieux d'agre\u0301ment, de repos, de\nre\u0302verie solitaire ou partage\u0301e, les jardins ont aussi e\u0301te\u0301 reve\u0302tus de\u0300s l'Antiquite\u0301\nd'une valeur symbolique. Ils apparaissent dans les mythologies et les religions, et\nils ont e\u0301te\u0301 fre\u0301quemment e\u0301voque\u0301s dans les arts.\n\n------------- chunk 1 ----------------\n\n# Le Jardin\n\n## E\u0301tymologie\n\nLe terme *jardin* est atteste\u0301 au [XIIe sie\u0300cle](https://fr.wikipedia.org/wiki/XIIe_si%C3%A8cle \"XIIe sie\u0300cle\") au sens de\n\u00ab terrain, ge\u0301ne\u0301ralement clos, ou\u0300 l'on cultive des ve\u0301ge\u0301taux utiles ou\nd'agre\u0301ment \u00bb (Grand mal fit Adam, e\u0301d. H. Suchier, 88). Il remonte a\u0300 un\ngallo-roman **hortus gardinus* (autrement *HORTU GARDINU,\n*gardinium* e\u0301tant atteste\u0301 au IXe sie\u0300cle en [latin me\u0301die\u0301val](https://fr.wikipedia.org/wiki/Latin_m%C3%A9di%C3%A9val \"Latin me\u0301die\u0301val\")), ce qui signifie litte\u0301ralement\n\u00ab jardin entoure\u0301 d'une clo\u0302ture \u00bb, compose\u0301 du latin *hortus*\n\u00ab jardin \u00bb et du [vieux bas francique](https://fr.wikipedia.org/wiki/Vieux-francique \"Vieux-francique\") **gart* ou **gardo*\n\u00ab clo\u0302ture \u00bb (d'ou\u0300 l'ancien franc\u0327ais *jart, gart* \u00ab jardin \u00bb\net le russe \u0433\u043e\u0440\u043e\u0434, *gorot*)[[1]](https://fr.wikipedia.org/wiki/Jardin#cite_note-%C3%A9tym_CNRTL-1). Le terme vieux bas francique,\nlangue non atteste\u0301e, est reconstitue\u0301 d'apre\u0300s le [vieux haut allemand](https://fr.wikipedia.org/wiki/Vieux_haut_allemand \"Vieux haut allemand\") *gart, garto*\n\u00ab jardin \u00bb, l'ancien ne\u0301erlandais *gaert* et le gotique\n*garda*[[1]](https://fr.wikipedia.org/wiki/Jardin#cite_note-%C3%A9tym_CNRTL-1). Cette e\u0301tymologie sugge\u0300re que le\njardin se doit d'e\u0302tre clos pour e\u0302tre prote\u0301ge\u0301 de l'exte\u0301rieur et bien entretenu a\u0300\nl'inte\u0301rieur.\n\nLe mot s'est diffuse\u0301 dans les autres langues romanes a\u0300 partir du franc\u0327ais\n*jardin*, ainsi qu'en anglais (*garden*) via le normand *gardin*,\nsemblable au picard *gardin*[[1]](https://fr.wikipedia.org/wiki/Jardin#cite_note-%C3%A9tym_CNRTL-1).\n</pre> In\u00a0[\u00a0]: Copied! <pre># Let's save the chunks. We can just pass the chunks we obtain and the filename we want\npipeline.save_chunks(chunks, \"mychunk.json\")\n</pre> # Let's save the chunks. We can just pass the chunks we obtain and the filename we want pipeline.save_chunks(chunks, \"mychunk.json\")"},{"location":"examples/html_chunking/#html-file-chunking","title":"HTML file chunking\u00b6","text":"<p>This notebook aims at showing a simple example of chunking for markdown (.md) files.</p>"},{"location":"examples/html_chunking/#pipeline-setup","title":"Pipeline setup\u00b6","text":""},{"location":"examples/html_chunking/#view-the-chunks","title":"View the chunks\u00b6","text":""},{"location":"examples/html_chunking/#save-the-chunks","title":"Save the chunks\u00b6","text":"<p>The pipeline as a method to save the chunks and their attributes as a json file. Here is how to use it.</p>"},{"location":"examples/markdown_chunking/","title":"Markdown file chunking","text":"In\u00a0[1]: Copied! <pre># If needed, install chunknorris\n%pip install chunknorris -q\n</pre> # If needed, install chunknorris %pip install chunknorris -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre># imported the required chunknorris components\nfrom chunknorris.parsers import MarkdownParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n</pre> # imported the required chunknorris components from chunknorris.parsers import MarkdownParser from chunknorris.chunkers import MarkdownChunker from chunknorris.pipelines import BasePipeline <p>3 components are needed to chunk a markdown file :</p> <ul> <li><p><code>MarkdownParser</code> : this parser ensures the formatting of the markdown file. In particular, it will :</p> <ul> <li>ensure that the headers are in ATX format</li> <li>detect tables, code blocks, metadata, ... to make sur they are not splitted across multiple chunks</li> </ul> <p>It returns a <code>MarkdownDoc</code> containing all information</p> </li> <li><p><code>MarkdownChunker</code> : the chunker takes as input a <code>MarkdownDoc</code> and performs chunking in multiple <code>Chunks</code> objects.</p> </li> <li><p><code>BasePipeline</code> : this pipline is pretty basic (you would have guessed considering its name... \ud83d\ude05). It just plugs together the parser and the chunker so that the output of the parser in fed to the chunker. Other pipelines, such as <code>PdfPipeline</code> handle more complex mechanics.</p> </li> </ul> In\u00a0[3]: Copied! <pre># instanciate\nparser = MarkdownParser()\nchunker = MarkdownChunker()\npipeline = BasePipeline(parser, chunker)\n</pre> # instanciate parser = MarkdownParser() chunker = MarkdownChunker() pipeline = BasePipeline(parser, chunker) In\u00a0[4]: Copied! <pre># Get those chunks !\npath_to_md_file = \"../../tests/test_files/file.md\"\nchunks = pipeline.chunk_file(path_to_md_file)\nprint(f\"Got {len(chunks)} chunks !\")\n</pre> # Get those chunks ! path_to_md_file = \"../../tests/test_files/file.md\" chunks = pipeline.chunk_file(path_to_md_file) print(f\"Got {len(chunks)} chunks !\") <pre>2024-12-17 09:59:ChunkNorris:INFO:Function \"chunk\" took 0.0068 seconds\n</pre> <pre>Got 17 chunks !\n</pre> In\u00a0[5]: Copied! <pre># Let's look at the chunks\nfor i, chunk in enumerate(chunks[:3]): # we only look at the 3 first chunks\n    print(f\"\\n------------- chunk {i} ----------------\\n\")\n    print(chunk.get_text())\n</pre> # Let's look at the chunks for i, chunk in enumerate(chunks[:3]): # we only look at the 3 first chunks     print(f\"\\n------------- chunk {i} ----------------\\n\")     print(chunk.get_text()) <pre>\n------------- chunk 0 ----------------\n\n# Jardin\n\nUn [jardin japonais](https://fr.wikipedia.org/wiki/Jardin_japonais \"Jardin japonais\").\n\nUne femme de 87 ans en train de cultiver son jardin. [Comte\u0301 de Harju](https://fr.wikipedia.org/wiki/Comt%C3%A9_de_Harju \"Comte\u0301 de Harju\"), Estonie, juin 2016\\.\n\nUn **jardin** est un lieu durablement et hypothe\u0301tiquement ame\u0301nage\u0301 ou\u0300 l'on cultive de fac\u0327on ordonne\u0301e des [plantes](https://fr.wikipedia.org/wiki/Plante \"Plante\") domestique\u0301es ou se\u0301lectionne\u0301es. Il est le produit de la technique du [jardinage](https://fr.wikipedia.org/wiki/Jardinage \"Jardinage\") et, comme elle, il remonte au moins a\u0300 l'Antiquite\u0301. Les diffe\u0301rentes cultures humaines dans le monde, au fil des e\u0301poques, ont invente\u0301 de nombreux types et styles de jardins. Lieux d'agre\u0301ment, de repos, de re\u0302verie solitaire ou partage\u0301e, les jardins ont aussi e\u0301te\u0301 reve\u0302tus de\u0300s l'Antiquite\u0301 d'une valeur symbolique. Ils apparaissent dans les mythologies et les religions, et ils ont e\u0301te\u0301 fre\u0301quemment e\u0301voque\u0301s dans les arts.\n\n------------- chunk 1 ----------------\n\n# Jardin\n\n## E\u0301tymologie\\[[modifier](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;veaction=edit&amp;section=1 \"Modifier la section : E\u0301tymologie\") \\| [modifier le code](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;action=edit&amp;section=1 \"Modifier le code source de la section : E\u0301tymologie\")]\n\nLe terme *jardin* est atteste\u0301 au [XIIe sie\u0300cle](https://fr.wikipedia.org/wiki/XIIe_si%C3%A8cle \"XIIe sie\u0300cle\") au sens de \u00ab terrain, ge\u0301ne\u0301ralement clos, ou\u0300 l'on cultive des ve\u0301ge\u0301taux utiles ou d'agre\u0301ment \u00bb (Grand mal fit Adam, e\u0301d. H. Suchier, 88\\). Il remonte a\u0300 un gallo\\-roman *\\*hortus gardinus* (autrement \\*HORTU GARDINU, *gardinium* e\u0301tant atteste\u0301 au IXe sie\u0300cle en [latin me\u0301die\u0301val](https://fr.wikipedia.org/wiki/Latin_m%C3%A9di%C3%A9val \"Latin me\u0301die\u0301val\")), ce qui signifie litte\u0301ralement \u00ab jardin entoure\u0301 d'une clo\u0302ture \u00bb, compose\u0301 du latin *hortus* \u00ab jardin \u00bb et du [vieux bas francique](https://fr.wikipedia.org/wiki/Vieux-francique \"Vieux-francique\") *\\*gart* ou *\\*gardo* \u00ab clo\u0302ture \u00bb (d'ou\u0300 l'ancien franc\u0327ais *jart, gart* \u00ab jardin \u00bb et le russe \u0433\u043e\u0440\u043e\u0434, *gorot*)[\\[1]](https://fr.wikipedia.org/wiki/Jardin#cite_note-%C3%A9tym_CNRTL-1). Le terme vieux bas francique, langue non atteste\u0301e, est reconstitue\u0301 d'apre\u0300s le [vieux haut allemand](https://fr.wikipedia.org/wiki/Vieux_haut_allemand \"Vieux haut allemand\") *gart, garto* \u00ab jardin \u00bb, l'ancien ne\u0301erlandais *gaert* et le gotique *garda*[\\[1]](https://fr.wikipedia.org/wiki/Jardin#cite_note-%C3%A9tym_CNRTL-1). Cette e\u0301tymologie sugge\u0300re que le jardin se doit d'e\u0302tre clos pour e\u0302tre prote\u0301ge\u0301 de l'exte\u0301rieur et bien entretenu a\u0300 l'inte\u0301rieur.\n\nLe mot s'est diffuse\u0301 dans les autres langues romanes a\u0300 partir du franc\u0327ais *jardin*, ainsi qu'en anglais (*garden*) via le normand *gardin*, semblable au picard *gardin*[\\[1]](https://fr.wikipedia.org/wiki/Jardin#cite_note-%C3%A9tym_CNRTL-1).\n\n------------- chunk 2 ----------------\n\n# Jardin\n\n## De\u0301finition\\[[modifier](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;veaction=edit&amp;section=2 \"Modifier la section : De\u0301finition\") \\| [modifier le code](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;action=edit&amp;section=2 \"Modifier le code source de la section : De\u0301finition\")]\n\nAu sens actuel un jardin comprend 3 composantes qui sont simultane\u0301ment pre\u0301sentes :\n\n1. la notion d'ame\u0301nagement durable de l'espace : dans, proche ou exte\u0301rieur a\u0300 l'habitation, clos ou de\u0301limite\u0301, ame\u0301nage\u0301 d'e\u0301quipements hydrauliques, d'entretien, d'aides a\u0300 la ve\u0301ge\u0301tation comme les pots horticoles, de circulations, de meubles, d'objets d'art, de constructions fonctionnelles ou de\u0301coratives. L'ame\u0301nagement a ge\u0301ne\u0301ralement un caracte\u0300re pe\u0301renne car les jardins sont spe\u0301cialement destine\u0301s a\u0300 cultiver des plantes vivaces, des arbres, qui sont durablement pre\u0301sents, mais les jardins peuvent aussi e\u0302tre saisonniers dans les pays a\u0300 saisons marque\u0301es, ou fugaces.\n2. la notion de culture des ve\u0301ge\u0301taux signifie que les ve\u0301ge\u0301taux ont pour le moins e\u0301te\u0301 se\u0301lectionne\u0301s intentionnellement, naturalise\u0301s, ou plus commune\u0301ment sont des plantes qui ont subi une [domestication](https://fr.wikipedia.org/wiki/Domestication \"Domestication\"), l'objet de nombreux jardins est l'e\u0301tude, la conservation ou la production de ve\u0301ge\u0301taux ou de sous produits ve\u0301ge\u0301taux. Ces ve\u0301ge\u0301taux peuvent e\u0302tre des [fleurs](https://fr.wikipedia.org/wiki/Fleur \"Fleur\"), des [le\u0301gumes](https://fr.wikipedia.org/wiki/Plante_potag%C3%A8re \"Plante potage\u0300re\"), des [arbres fruitiers](https://fr.wikipedia.org/wiki/Arbre_fruitier \"Arbre fruitier\") ou d\u2019[ornement](https://fr.wikipedia.org/wiki/Arboretum \"Arboretum\"), des plantes me\u0301dicinales ou utilitaires. Le terme jardin est e\u0301galement accepte\u0301 pour un espace clos constituant un de\u0301cor entie\u0300rement mine\u0301ral typique du [jardin japonais](https://fr.wikipedia.org/wiki/Jardin_japonais \"Jardin japonais\"), ou\u0300 la notion de culture des ve\u0301ge\u0301taux est repousse\u0301e a\u0300 l'environnement.\n3. la notion d'ordonnance qui signifie qu'un jardin est toujours organise\u0301. Philippe Descola, observant les Indiens Achuar, de\u0301finit le jardin comme une \u00ab anti\\-fore\u0302t \u00bb, exhibant la \u00ab mai\u0302trise dans la destruction du naturel \u00bb[\\[2]](https://fr.wikipedia.org/wiki/Jardin#cite_note-2).\n\nLes jardins sont atteste\u0301s dans toutes les zones de domestication des plantes ou\u0300 ils sont des lieux plus sophistique\u0301s que les champs ou les pre\u0301s. La question du lien entre jardin et se\u0301dentarisation est complexe, il existe des formes rudimentaires de jardins chez les nomades... alors de nombreux se\u0301dentaires ne jardinent pas et be\u0301tonnent leur jardinets. La re\u0300gle esthe\u0301tique quasi unique des jardins de climat me\u0301diterrane\u0301en e\u0301tait, depuis les origines, l'alignement. Elle est reste\u0301e inchange\u0301e jusqu'au XIXe sie\u0300cle dans les jardins perses. La re\u0300gle des jardins chinois est l'e\u0301vocation. L'ordonnance des vergers et potagers en planches ou carre\u0301s est justifie\u0301e par les besoins d'entretien, sanitaire et de production.\n\nUne [friche](https://fr.wikipedia.org/wiki/Friche \"Friche\") n'est pas un jardin, me\u0302me si elle est un ancien jardin a\u0300 l'abandon. La notion de jardin suppose un minimum d'attention, y compris pour ce qu'il est convenu d'appeler jardin sauvage ou naturel qui sont des jardins ou\u0300 on laisse pousser des plantes spontane\u0301es.\n</pre> <p>You may want to remove the links in a chunk by using <code>Chunk.get_text(remove_links=True)</code>.</p> <p>The <code>Chunk.get_text()</code> method allows to to directly concatenate the headers of all top-level sections with the chunk's content. If you want to customize this behavior, you may use the <code>Chunk.headers</code> and <code>Chunk.content</code> attributes. Both contain <code>MarkdownLine</code> objects, which represent a markdown line of the file and its metadata.</p> In\u00a0[6]: Copied! <pre># Let's see a chunk in details (chunk number 10 for example)\nfor line in chunks[10].headers:\n    print(line)\nprint(\"=======================\")\nfor line in chunks[10].content:\n    print(line)\n</pre> # Let's see a chunk in details (chunk number 10 for example) for line in chunks[10].headers:     print(line) print(\"=======================\") for line in chunks[10].content:     print(line) <pre>{'text': '# Jardin', 'line_idx': 0, 'isin_code_block': False, 'page': None}\n{'text': '## Les jardins en France\\\\[[modifier](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;veaction=edit&amp;section=10 \"Modifier la section\\u202f: Les jardins en France\") \\\\| [modifier le code](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;action=edit&amp;section=10 \"Modifier le code source de la section : Les jardins en France\")]', 'line_idx': 99, 'isin_code_block': False, 'page': None}\n{'text': '### Protection \u00e0 titre patrimonial de certains parcs et jardins\\\\[[modifier](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;veaction=edit&amp;section=11 \"Modifier la section\\u202f: Protection \u00e0 titre patrimonial de certains parcs et jardins\") \\\\| [modifier le code](https://fr.wikipedia.org/w/index.php?title=Jardin&amp;action=edit&amp;section=11 \"Modifier le code source de la section : Protection \u00e0 titre patrimonial de certains parcs et jardins\")]', 'line_idx': 113, 'isin_code_block': False, 'page': None}\n=======================\n{'text': '', 'line_idx': 114, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 115, 'isin_code_block': False, 'page': None}\n{'text': 'Grotte dans le parc du vicomte de Bona\\\\-Dona (\u00e9poque style [baroque](https://fr.wikipedia.org/wiki/Baroque \"Baroque\")), en r\u00e9gion Champagne\\\\-Ardenne (France).', 'line_idx': 116, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 117, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 118, 'isin_code_block': False, 'page': None}\n{'text': 'Fin 2007[\\\\[9]](https://fr.wikipedia.org/wiki/Jardin#cite_note-9), 2\\xa0034 parcs et jardins \u00e9taient prot\u00e9g\u00e9s (dont 579 class\u00e9s et 1\\xa0455 inscrits) au titre de la loi du 31 d\u00e9cembre 1913 des [monuments historiques](https://fr.wikipedia.org/wiki/Monument_historique_(France) \"Monument historique (France)\"). Les propri\u00e9taires (quoique les jardins puissent faire l\u2019objet de propri\u00e9t\u00e9s multiples) sont surtout priv\u00e9s. La campagne \u00ab\\xa0Visitez un jardin en France\\xa0\u00bb a \u00e9t\u00e9 l\u2019occasion de d\u00e9couvrir des jardins priv\u00e9s ou publics, historiques ou contemporains, parcs floraux ou botaniques. Le souci d\u2019une meilleure connaissance des jardins et celui de leur caract\u00e8re historique se sont r\u00e9v\u00e9l\u00e9s beaucoup plus tard que celui des \u00e9l\u00e9ments plus \u00ab\\xa0classiques\\xa0\u00bb du patrimoine\\xa0: monuments, objets d\u2019art. Le terme de jardin n\u2019appara\u00eet pratiquement pas dans les arr\u00eat\u00e9s de protection du XIXe\\xa0si\u00e8cle et tr\u00e8s peu avant 1920\\\\. C\u2019est m\u00eame plus g\u00e9n\u00e9ralement des ann\u00e9es 1930 que datent beaucoup d\u2019arr\u00eat\u00e9s de protection de parcs ou de jardins. Ils sont alors identifi\u00e9s, nomm\u00e9s, mais rarement d\u00e9crits.', 'line_idx': 119, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 120, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 121, 'isin_code_block': False, 'page': None}\n{'text': 'En compl\u00e9ment de la protection au titre des \"Monuments Historiques\", le Minist\u00e8re de la Culture et de la Communication a cr\u00e9\u00e9 en 2004, sur la proposition du Comit\u00e9 national des Parcs et Jardins un label \"Jardin remarquable\"[\\\\[10]](https://fr.wikipedia.org/wiki/Jardin#cite_note-10) dont l\u2019objet est de distinguer des jardins et des parcs, publics ou priv\u00e9s, pr\u00e9sentant un int\u00e9r\u00eat culturel, esth\u00e9tique, historique ou encore botanique. Ces jardins et parcs doivent \u00eatre ouverts au public au moins 40 jours par an. Ce label, gage de qualit\u00e9, est attribu\u00e9 pour 5 ans. En 2017, 407 parcs et jardins revendiquent ce label, en France et dans les DOM.', 'line_idx': 122, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 123, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 124, 'isin_code_block': False, 'page': None}\n{'text': 'Chaque premier week\\\\-end de [juin](https://fr.wikipedia.org/wiki/Juin \"Juin\") depuis [2003](https://fr.wikipedia.org/wiki/2003 \"2003\"), le [Minist\u00e8re fran\u00e7ais de la Culture et de la Communication](https://fr.wikipedia.org/wiki/Minist%C3%A8re_de_la_Culture_(France) \"Minist\u00e8re de la Culture (France)\") organise un \u00e9v\u00e9nement \u00e0 l\\'\u00e9chelle nationale\\xa0: [Rendez\\\\-vous aux jardins](https://fr.wikipedia.org/wiki/Rendez-vous_aux_jardins \"Rendez-vous aux jardins\"), durant lequel de nombreux jardins sont ouverts au public.', 'line_idx': 125, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 126, 'isin_code_block': False, 'page': None}\n{'text': '', 'line_idx': 127, 'isin_code_block': False, 'page': None}\n</pre> In\u00a0[\u00a0]: Copied! <pre># Let's save the chunks. We can just pass the chunks we obtain and the filename we want\npipeline.save_chunks(chunks, \"mychunk.json\")\n</pre> # Let's save the chunks. We can just pass the chunks we obtain and the filename we want pipeline.save_chunks(chunks, \"mychunk.json\")"},{"location":"examples/markdown_chunking/#markdown-file-chunking","title":"Markdown file chunking\u00b6","text":"<p>This notebook aims at showing a simple example of chunking for markdown (.md) files.</p>"},{"location":"examples/markdown_chunking/#pipeline-setup","title":"Pipeline setup\u00b6","text":""},{"location":"examples/markdown_chunking/#view-the-chunks","title":"View the chunks\u00b6","text":""},{"location":"examples/markdown_chunking/#save-the-chunks","title":"Save the chunks\u00b6","text":"<p>The pipeline as a method to save the chunks and their attributes as a json file. Here is how to use it.</p>"},{"location":"examples/pdf_chunking/","title":"PDF file chunking","text":"In\u00a0[\u00a0]: Copied! <pre># If needed, install chunknorris\n%pip install chunknorris -q\n</pre> # If needed, install chunknorris %pip install chunknorris -q In\u00a0[1]: Copied! <pre>from chunknorris.parsers import PdfParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\nfrom IPython.display import Markdown\n</pre> from chunknorris.parsers import PdfParser from chunknorris.chunkers import MarkdownChunker from chunknorris.pipelines import BasePipeline from IPython.display import Markdown In\u00a0[4]: Copied! <pre># Setup the pipe. Feel free to play with the parser and chunker's arguments.\npipeline = BasePipeline(\n    PdfParser(),\n    MarkdownChunker(),\n)\n\nchunks = pipeline.chunk_file(\"./example_data/sample.pdf\")\nprint(f\"Got {len(chunks)} chunks !\")\n</pre> # Setup the pipe. Feel free to play with the parser and chunker's arguments. pipeline = BasePipeline(     PdfParser(),     MarkdownChunker(), )  chunks = pipeline.chunk_file(\"./example_data/sample.pdf\") print(f\"Got {len(chunks)} chunks !\") <pre>2025-07-01 15:39:ChunkNorris:INFO:Function \"get_tables\" took 1.4941 seconds\n2025-07-01 15:39:ChunkNorris:INFO:Function \"parse_file\" took 2.3959 seconds\n2025-07-01 15:39:ChunkNorris:INFO:Function \"chunk\" took 0.0955 seconds\n</pre> <pre>Got 218 chunks !\n</pre> <p>As we can see, the chunking of this 165 pages documents took around:</p> <ul> <li>2.4s for parsing (including 1.3s for parsing the tables)</li> <li>0.1s for chunking.</li> </ul> <p>--&gt; around 2.5s total</p> <p>It led to 218 chunks.</p> <p>(Hardware : CPU - i7-13620H, 2.40 GHz, RAM - 16 Go)</p> In\u00a0[5]: Copied! <pre>for chunk_idx in [10, 11]: # choose any\n    chunk = chunks[chunk_idx]\n    print(f\"\\n===== Start page: {chunk.start_page} --- End page: {chunk.end_page} ======\\n\")\n    print(chunk.get_text())\n</pre> for chunk_idx in [10, 11]: # choose any     chunk = chunks[chunk_idx]     print(f\"\\n===== Start page: {chunk.start_page} --- End page: {chunk.end_page} ======\\n\")     print(chunk.get_text()) <pre>\n===== Start page: 9 --- End page: 11 ======\n\n## **Welcome**\n\n### 2.3 Phone Features\n\nThe following table describes the IP Phone features:\nUser Guide 4\n**Welcome**\n|  Feature  |  6930 IP Phone  |  6930w IP Phone  |\n|:---|:---|:---|\n| Display  | 4.3\u201d WQVGA (480x272) color TFT LCD display with brightness controls  | 4.3\" WQVGA (480x272) color TFT LCD display with brightness controls  |\n| Programmable Keys  | 12 top softkeys  | 12 top softkeys  |\n| Context Sensitive Keys  | 5 context-sensitive bottom softkeys  | 5 context-sensitive bottom softkeys  |\n| Ethernet  | Built-in-two-port, 10/100/1000 Gigabit Ethernet switch - lets you share a connection with your computer | Built-in-two-port, 10/100/1000 Gigabit Ethernet switch - lets you share a connection with your computer 802.3az (EEE)  |\n| Power-over-Ethernet (PoE) - LAN | 802.3af, 802.3at  | 802.3af, 802.3at  |\n| POE Class  | Class 3 with auto change to 4 when PKMs are attached.  | Class 3 with auto change to 4 when PKMs are attached. If an accessory is installed in the sidecar accessory port, the phone must be powered using a 48v power brick. |\n| Bluetooth Support  | Embedded Bluetooth 4.1  | Embedded Bluetooth 5.2  |\n| External USB Port  | 1x USB 2.0 (100mA) Host  | 1x USB 2.0 (500mA) Host  |\n| PC Link / Mobile Link  | Yes  | Yes  |\n| 802.11n Wi-Fi  | -  | Yes (built-in)  |\n| Antimicrobial Plastics  | No  | No  |\n| DHSG Headset Support (H20/40)  | Yes  | Yes  |\n5 User Guide\n**Welcome**\n|  Feature  |  6930 IP Phone  |  6930w IP Phone  |\n|:---|:---|:---|\n| USB Headset Support (H10/30/40)  | Yes  | Yes  |\n| S720 BT Speakerphone  | Yes  | Yes  |\n| Integrated DECT Headset  | Yes  | Yes  |\n| M695 Programmable Key Module  | Yes (3 max)  | Yes (3 max)  |\n| Press-and-hold Speed dial key configuration feature | Yes  | Yes  |\n| Call Lines  | Supports up to 24 call lines with LEDs | Supports up to 24 call lines with LEDs |\n| AC power adapter  | Yes. Sold separately  | Yes. Sold separately  |\n| Supports Cordless Bluetooth handset  | Yes  | Yes  |\n**Note**  :\nThe  **6930L**  and  **6930Lt**  IP Phone variants do not contain Bluetooth circuitry and so do not support the related wireless functions. Any information within this document related to radio performance or functionality only relates to the fully functional 6930 IP Phone with Bluetooth capability.\n\n===== Start page: 11 --- End page: 12 ======\n\n## **Welcome**\n\n### 2.4 Requirements\nThe 6930 requires the following environment:\n- SIP-based IP PBX system or network installed and running with a SIP account created for the 6930\n\n phone\n- Access to a Trithroughl File Transfer Protocol (TFTP), File Transfer Protocol (FTP), Hypertext Transfer\n\n Protocol (HTTP) server, or Hyper Text Transfer Protocol over Secure Sockets Layer (SSL) (HTTPS)\nUser Guide 6\n- Ethernet/Fast Ethernet LAN (10/100 Mbps) (Gigabit Ethernet LAN [1000 Mbps] recommended)\n- Category 5/5e straight-through cabling (Category 6 straight-through cabling required for optimum\n\n Gigabit Ethernet performance)\n- Power source:\n- For Ethernet networks that supply inline power to the phone (IEEE 802.3af) use an Ethernet cable to\n\n connect from the phone directly to the network for power (no 48V AC power adapter required if using Power-over-Ethernet [PoE])\n- For Ethernet networks that DO NOT supply power to the phone:\n- Use only the GlobTek Inc. Limited Power Source [LPS] adapter model no. GT-41080-1848(sold\n\n separately) to connect from the DC power port on the phone to a power source or\n- Use a PoE power injector or a PoE switch\n</pre> In\u00a0[16]: Copied! <pre>pipeline.save_chunks(chunks, \"mychunks.json\")\n</pre> pipeline.save_chunks(chunks, \"mychunks.json\")"},{"location":"examples/pdf_chunking/#pdf-file-chunking","title":"PDF file chunking\u00b6","text":"<p>This notebook aims at showing a simple example of chunking for PDF files.</p> <p>Note: You may want to have a look at the tutorial In-depth .pdf file parsing to get more info about the functionnalities of the <code>PdfParser</code>.</p>"},{"location":"examples/pdf_chunking/#pipeline-setup","title":"Pipeline setup\u00b6","text":""},{"location":"examples/pdf_chunking/#view-the-chunks","title":"View the chunks\u00b6","text":"<p>To look at the chunk's text, you may use the <code>Chunk.get_text()</code> method.</p> <p>Another thing : for pdf file chunking, each chunk contains information about the pages this chunk comes from.</p>"},{"location":"examples/pdf_chunking/#save-the-chunks","title":"Save the chunks\u00b6","text":"<p>In order to save the chunks in a JSON file, just use this:</p>"},{"location":"examples/pdf_parsing/","title":"In-depth .pdf file parsing","text":"In\u00a0[3]: Copied! <pre># If needed, install chunknorris\n%pip install chunknorris -q\n</pre> # If needed, install chunknorris %pip install chunknorris -q <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[12]: Copied! <pre>from chunknorris.parsers import PdfParser\nfrom IPython.display import Markdown\n</pre> from chunknorris.parsers import PdfParser from IPython.display import Markdown In\u00a0[13]: Copied! <pre># Use the following block to parse a pdf file from an url\n\n# import requests\n# r = requests.get(\"myurl.pdf\")\n# data = r.content\n# parser = PdfParser()\n# parsed_doc = parser.parse_string(data)\n</pre> # Use the following block to parse a pdf file from an url  # import requests # r = requests.get(\"myurl.pdf\") # data = r.content # parser = PdfParser() # parsed_doc = parser.parse_string(data) In\u00a0[30]: Copied! <pre>path_to_pdf = \"./data/sample.pdf\" # Mitel phones user manual, 265 pages.\n\n# Instanciate parser and parse (should take around 2s)\nparser = PdfParser()\nparsed_doc = parser.parse_file(path_to_pdf)\n</pre> path_to_pdf = \"./data/sample.pdf\" # Mitel phones user manual, 265 pages.  # Instanciate parser and parse (should take around 2s) parser = PdfParser() parsed_doc = parser.parse_file(path_to_pdf) <pre>2024-12-16 14:01:ChunkNorris:INFO:Function \"_create_spans\" took 0.4723 seconds\n2024-12-16 14:01:ChunkNorris:INFO:Function \"get_tables\" took 1.3617 seconds\n2024-12-16 14:01:ChunkNorris:INFO:Function \"parse_file\" took 2.2170 seconds\n</pre> <p>As we can see, the total time elapsed to parse the 265 pages file is around 2s ! (including 1.3s to parse the table, and 0.47s to parse the text). This will vary depending on your hardware, the amount of tables in the document or the need to do OCR.</p> <p>If you are certain your documents do not contain tables, or do not need OCR, you may use the following code to make it even faster :</p> <pre>parser = PdfParser(\n    extract_tables=False,\n    use_ocr=\"never\"\n)\n</pre> In\u00a0[54]: Copied! <pre># Let's view a sample of the document\nmd_string = parsed_doc.to_string()\nMarkdown(\n    \"___________\\n[...]\" + md_string[26400:34000] + \"[...]\\n_______________\") \n</pre> # Let's view a sample of the document md_string = parsed_doc.to_string() Markdown(     \"___________\\n[...]\" + md_string[26400:34000] + \"[...]\\n_______________\")  Out[54]: <p>[...]Phone also supports the same accessories as the current 6930 IP Phone.**</p> <p>As we can see, the output markdown looks fine. The title have been recognized and converted as markdown headers, as well as the tables.</p> <p>Concerning the headers, <code>chunknorris</code> will attenmpt to find the table of content :</p> <ul> <li>In the file's metadata.</li> <li>If not available it will attempt to find it in the document using regex</li> <li>If still not available it will infer the headers based to font size.</li> </ul> In\u00a0[62]: Copied! <pre># Let's diplay the first elements of the table of content :\nprint(f\"Amount of TOC items detected : {len(parser.toc)}.\\nSample : \")\nparser.toc[:4]\n</pre> # Let's diplay the first elements of the table of content : print(f\"Amount of TOC items detected : {len(parser.toc)}.\\nSample : \") parser.toc[:4] <pre>Amount of TOC items detected : 139.\nSample : \n</pre> Out[62]: <pre>[TocTitle(text='Contents', source='metadata', page=3, level=1, x_offset=None, source_page=None, found=True),\n TocTitle(text='What\u2019s New', source='metadata', page=7, level=1, x_offset=None, source_page=None, found=True),\n TocTitle(text='Welcome', source='metadata', page=8, level=1, x_offset=None, source_page=None, found=True),\n TocTitle(text='About this Guide', source='metadata', page=10, level=2, x_offset=None, source_page=None, found=True)]</pre> In\u00a0[78]: Copied! <pre># You may also want to look at the tables detected\nprint(f\"Amount of tables detected : {len(parser.tables)}\")\ntable_idx = 2 # Choose the idx you want\nMarkdown(parser.tables[table_idx].to_markdown())\n</pre> # You may also want to look at the tables detected print(f\"Amount of tables detected : {len(parser.tables)}\") table_idx = 2 # Choose the idx you want Markdown(parser.tables[table_idx].to_markdown()) <pre>Amount of tables detected : 84\n</pre> Out[78]: Feature 6930 IP Phone 6930w IP Phone USB Headset Support (H10/30/40) Yes Yes S720 BT Speakerphone Yes Yes Integrated DECT Headset Yes Yes M695 Programmable Key Module Yes (3 max) Yes (3 max) Press-and-hold Speed dial key configuration feature Yes Yes Call Lines Supports up to 24 call lines with LEDs Supports up to 24 call lines with LEDs AC power adapter Yes. Sold separately Yes. Sold separately Supports Cordless Bluetooth handset Yes Yes <p>To parse tables, <code>chunknorris</code> uses the \"lines\" visible on the pages. Please take note that the tables that have a suggested structure (i.e. no lines) may not be detected as tables. Most of the time, that won't be a problem as such tables displayed as text will still be well undersood by LLMs.</p> In\u00a0[83]: Copied! <pre># Let's plot the parsed pdf\nparser.plot_pdf(page_start=75, page_end=78, dpi=100) # avoid displaying more that 20 pages at a time as matplotlib uses lots of RAM\n</pre> # Let's plot the parsed pdf parser.plot_pdf(page_start=75, page_end=78, dpi=100) # avoid displaying more that 20 pages at a time as matplotlib uses lots of RAM <p>The plotted pdf elements are :</p> <ul> <li>headers/footers in red</li> <li>tables in blue</li> <li>text in yellow including :<ul> <li>text lines surrounded by black thin lines</li> <li>text blocks (= group of lines) surrounded with dashed black lines</li> </ul> </li> </ul>"},{"location":"examples/pdf_parsing/#in-depth-pdf-file-parsing","title":"In-depth .pdf file parsing\u00b6","text":"<p>This tutorial shows you how to use the <code>PdfParser</code> efficiently.</p>"},{"location":"examples/pdf_parsing/#get-the-markdown-string","title":"Get the markdown string\u00b6","text":""},{"location":"examples/pdf_parsing/#21-about-this-guide","title":"2.1 About this Guide\u00b6","text":"<p>This guide explains how to use the basic features of your new 6390SIP IP phone. Not all features listed are available by default. Contact your System or Network Administrator to find out which features and services are available to you on your system. Your System Administrator has the ability to customize some features on this phone. For information on more advanced settings and configurations, Administrators Sshould refer to the  Mitel SIP IP Phones Administrator Guide  .</p>"},{"location":"examples/pdf_parsing/#22-documentation","title":"2.2 Documentation\u00b6","text":"<ul> <li>Mitel 6930 SIP IP Phone Quick Reference Guide  -  Contains call handling instructions, an overview of the User Interface (UI) and details on UI navigation, as well as information on other important features. The Quick Reference Guide can be downloaded from http://www.miteldocs.com .</li> <li>Mitel 6930 IP Phone Installation Guide  -  Contains installation and set-up instructions, general features and functions, and basic options list customization. The Installation Guide can be downloaded from http://www.miteldocs.com .</li> <li>Mitel 6900 SIP IP Phones Administrator Guide  -  Describes how to set up the Mitel SIP IP phones on the network and contains advanced configuration instructions. The Administrator Guide is intended for the System Administrator and can be downloaded from http://www.miteldocs.com .</li> </ul>"},{"location":"examples/pdf_parsing/#23-phone-features","title":"2.3 Phone Features\u00b6","text":"<p>The following table describes the IP Phone features: User Guide 4 Welcome</p> Feature 6930 IP Phone 6930w IP Phone Display 4.3\u201d WQVGA (480x272) color TFT LCD display with brightness controls 4.3\" WQVGA (480x272) color TFT LCD display with brightness controls Programmable Keys 12 top softkeys 12 top softkeys Context Sensitive Keys 5 context-sensitive bottom softkeys 5 context-sensitive bottom softkeys Ethernet Built-in-two-port, 10/100/1000 Gigabit Ethernet switch - lets you share a connection with your computer Built-in-two-port, 10/100/1000 Gigabit Ethernet switch - lets you share a connection with your computer 802.3az (EEE) Power-over-Ethernet (PoE) - LAN 802.3af, 802.3at 802.3af, 802.3at POE Class Class 3 with auto change to 4 when PKMs are attached. Class 3 with auto change to 4 when PKMs are attached. If an accessory is installed in the sidecar accessory port, the phone must be powered using a 48v power brick. Bluetooth Support Embedded Bluetooth 4.1 Embedded Bluetooth 5.2 External USB Port 1x USB 2.0 (100mA) Host 1x USB 2.0 (500mA) Host PC Link / Mobile Link Yes Yes 802.11n Wi-Fi - Yes (built-in) Antimicrobial Plastics No No DHSG Headset Support (H20/40) Yes Yes <p>5 User Guide Welcome</p> Feature 6930 IP Phone 6930w IP Phone USB Headset Support (H10/30/40) Yes Yes S720 BT Speakerphone Yes Yes Integrated DECT Headset Yes Yes M695 Programmable Key Module Yes (3 max) Yes (3 max) Press-and-hold Speed dial key configuration feature Yes Yes Call Lines Supports up to 24 call lines with LEDs Supports up to 24 call lines with LEDs AC power adapter Yes. Sold separately Yes. Sold separately Supports Cordless Bluetooth handset Yes Yes <p>Note  : The  6930L  and  6930Lt  IP Phone variants do not contain Bluetooth circuitry and so do not support the related wireless functions. Any information within this document related to radio performance or functionality only relates to the fully functional 6[...]</p>"},{"location":"examples/pdf_parsing/#observe-table-of-content","title":"Observe table of content\u00b6","text":""},{"location":"examples/pdf_parsing/#observe-the-detected-tables","title":"Observe the detected tables\u00b6","text":""},{"location":"examples/pdf_parsing/#visualize-the-parsed-pdf","title":"Visualize the parsed pdf\u00b6","text":"<p>You may want to \"plot\" the parsed pdf file. This can help you :</p> <ul> <li>debug</li> <li>visualize which tables have been detected</li> <li>visualize which elements have been detected has page headers/footers</li> <li>...</li> </ul>"},{"location":"pages/current_limitations/","title":"Current limitations","text":"<p>This sections covers some examples about <code>chunknorris</code>'s know current limitations.</p> Note <p>If you've ever worked with parsing documents, you'll know that producing clean results can be quite challenging, particularly with .pdf files. The diverse range of document layouts makes it difficult to create a universal solution that works seamlessly for all documents. </p> <p>As a result, even though <code>chunknorris</code> has been developed to consistently deliver reliable results across various documents, there may still be instances where it does not perform as expected.</p>"},{"location":"pages/current_limitations/#pdf-table-detection-for-borderless-tables","title":"[PDF] Table detection for borderless tables","text":"<p><code>chunknorris</code> uses the vectors (= lines) present in <code>.pdf</code> to parse the table's structure and infer the table's cells. While this works great, it also means that tables that only have a suggested structure (no lines) won't be parsed properly as tables.</p> Example of tables correctly parsed Input Output Example of tables incorrectly parsed Input Output <p>Tip</p> <p>The fact that a given table is not parsed properly does not mean that the LLM will not be able to answer ! LLM have seen plently of data like this and may be able to \"guess\" the data structure.</p> <p>If your documents are packed with borderless tables and their parsing is crucial to you, I recommend managing table parsing separately with one of the many available tools or AI models specifically designed for this purpose. As most of the best performing tools rely on deep learning, choice has been made to avoid breaking speed performance by integrating such tools in <code>chunknorris</code>. Nevertheless, some work is planned to achieve conparable results with minimal computational requirements.</p>"},{"location":"pages/current_limitations/#pdf-wrong-reading-order-for-pdf-coming-from-powerpoint","title":"[PDF] Wrong reading order for .pdf coming from Powerpoint","text":"<p>Because of the manner text blocks are encoded in the .pdf file when exported from powerpoint, the resulting text order may not reflect the intuitive reading order. This issue is not specific to <code>chunknorris</code>: almost every library that offer to extract the text from a pdf file will have this same issue !"},{"location":"pages/faq/","title":"Frequently asked questions","text":"<p>\ud83d\udea7 Work in progress \ud83d\udea7</p>"},{"location":"pages/getting_started/","title":"Getting started","text":"<p>So far, chunknorris contains implementation for chunking the following file types : .md, .html or .pdf. </p> <p>After you install <code>chunknorris</code>, you can start getting your chunks either using python or the cli.</p>"},{"location":"pages/getting_started/#using-python","title":"Using Python","text":"<p>Here are code examples for chunking documents according to their file type:</p> .md.html.pdf.docx.csv.xlsx.ipynb <pre><code>from chunknorris.parsers import MarkdownParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=MarkdownParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.md\")\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <pre><code>from chunknorris.parsers import HTMLParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=HTMLParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.html\")\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <pre><code>from chunknorris.parsers import PdfParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=PdfParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.pdf\")\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <pre><code>from chunknorris.parsers import DocxParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=DocxParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.docx\")\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <pre><code>from chunknorris.parsers import CSVParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=CSVParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.csv\")\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <pre><code>from chunknorris.parsers import ExcelParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=ExcelParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.xslx\") #  .xls, .odt or any excel-like file is compatible.\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <pre><code>from chunknorris.parsers import JupyterNotebookParser\nfrom chunknorris.chunkers import MarkdownChunker\nfrom chunknorris.pipelines import BasePipeline\n\n# Instanciate components\npipeline = BasePipeline(\n    parser=JupyterNotebookParser(),\n    chunker=MarkdownChunker()\n    )\n\n# Get some chunks !\nchunks = pipeline.chunk_file(filepath=\"myfile.ipynb\")\n\n# Save chunks\npipeline.save_chunks(chunks)\n\n# Print chunks:\nfor chunk in chunks:\n    print(chunk.get_text())\n</code></pre> <p>Once you have your <code>chunks</code>, you can easily print them of save them:</p>"},{"location":"pages/getting_started/#using-the-cli","title":"Using the CLI","text":"<p>If you prefer, you may also run <code>chunknorris</code> in a terminal using:</p> <pre><code>chunknorris --filepath \"path/to/myfile.pdf\" # .pdf, .md or .html\n</code></pre> <p>In that case, the chunks will be saved in a JSON file, named <code>&lt;name_of_you_file&gt;-chunks.json</code>.</p> <p>You may also pass arguments to influence <code>chunknorris</code>' behavior. Enter <code>chunknorris -h</code> in a terminal to see available options. Feel free to experiment \ud83e\uddea !</p>"},{"location":"pages/how_it_works/","title":"\u2699\ufe0f How it works","text":"<p>In a nutshell, <code>chunknorris</code> relies on 3 components :</p> <ul> <li>Parsers : they handle the cleaning and formatting of the input document.</li> <li>Chunkers : they use the output of the parser and handle its chunking.</li> <li>Pipelines: they combine a parser and a chunker, allowing to output chunks directly from you input documents.</li> </ul>"},{"location":"pages/how_it_works/#parsers","title":"Parsers","text":"<p>The role of parsers is to take a file or a string as input, and output a clean markdown-formatted content suited for a chunker. Each parser is dedicated to a type of document. </p> <p>Various parsers are available, for example :</p> <ul> <li> <p><code>MarkdownParser</code> : for parsing markdown files/strings.</p> </li> <li> <p><code>HTMLParser</code> : for parsing html-formatted files/strings.</p> </li> <li> <p><code>PdfParser</code> : for parsing PDF files.</p> </li> </ul> <p>All parsers will output a markdown-formatted <code>MarkdownDoc</code> object, whose puprose is to be fed to a chunker.</p>"},{"location":"pages/how_it_works/#chunkers","title":"Chunkers","text":"<p>The role of chunkers is to process the output of parsers in order to obtain relevant chunks from the document. As all parsers output markdown-formatted <code>MarkdownDoc</code> objects, all their outputs can be chunked by the <code>MarkdownChunker</code>.</p> Note <p>Markdown formatting has proven to be very well understood by LLMs, while being human-readable (unlike HTML for example). Consequently, it has been chosen as a common interface for all parsers' outputs. That is why only the <code>MarkdownChunker</code> has been developed so far. More chunkers could be developed in the future if there is the need to chunk other types of parser's output.</p> <p>The chunking strategy of chunkers is based on several principles :</p> <ul> <li> <p>Each chunk must carry homogenous information. To this end, chunkers use the document's headers to chunk the documents. It helps ensuring that a specific piece of information is not splitted across multiple chunks.</p> </li> <li> <p>Each chunk must keep contextual information. A document's section might loose its meaning if the reader as no knowledge of its context. Consequently, all the headers of the parents sections are added ad the top of the chunk.</p> </li> <li> <p>All chunks must be of similar sizes. Indeed, when attempting to retrieve relevant chunks regarding a query, embedding models tend to be sensitive to the length of chunks. Actually, it is likely that a chunk with a text content of similar length to the query will have a high similarity score, while a chunk with a longer text content will see its similarity score descrease despite its relevancy. To prevent this, chunkers try to keep chunks of similar sizes whenever possible.</p> </li> </ul> <p>All chunkers output a list of <code>Chunk</code> objects containing the chunk's text and information relative to it.</p> <p></p>"},{"location":"pages/how_it_works/#pipelines","title":"Pipelines","text":"<p>Pipelines are the glue that sticks together a parser and a chunker. They use both to process documents and ensure constant output quality.</p> <p></p>"},{"location":"pages/how_to_contribute/","title":"Contributing","text":"<p>\ud83d\udea7 Work in progress \ud83d\udea7</p>"},{"location":"pages/installation/","title":"\u2b07\ufe0f Installation","text":"<p>You can simply install chunknorris in your python environment by using pip. You may as well want to install from source in order to get the latest developments.</p> Pipfrom source <pre><code>pip install chunknorris\n</code></pre> <pre><code>git clone https://github.com/wikit-ai/chunknorris.git\ncd chunknorris\npip install .\n</code></pre>"},{"location":"pages/installation/#development-setup","title":"\ud83e\udd1d Development setup","text":"<p>Every contribution to <code>chunknorris</code> is welcome ! If you which to develop extra features, or just fix a bug, just clone the repository to your machine and install the extra dev dependencies : </p> <pre><code>git clone https://github.com/wikit-ai/chunknorris.git\ncd chunknorris\npip install \".[dev]\"\n</code></pre>"},{"location":"pages/performance_report/","title":"Performance report","text":"<p>\ud83d\udea7 Work in progress \ud83d\udea7</p>"},{"location":"pages/roadmap/","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Here are the ongoing developments for <code>chunknorris</code>, or features that have been identified for future development.</p> <p>Each feature is developed with <code>chunknorris</code>' main motivation in mind: finding fast methods that require minimal computing resources. \ud83c\udf31 (Sorry dear machine learning...)</p> Feature Status [PDF] Parsing of tables delimited by lines in .pdf documents \ud83d\udfe2 [DOCX] Parsing of .docx documents to markdown \ud83d\udfe1 [PDF] Title tree detection when no table of contents is present in the document \ud83d\udfe1 [PDF] Fix typos induced by tesseract (c.f. note 1) \ud83d\udfe1 <p>\ud83d\udd35 = ongoing \ud83d\udfe1 = in focus \ud83d\udfe2 = recently done</p> <p>Note 1 : When using OCR on pdf documents, the backend used (Tesseract) may introduce typos. Some other tools, such as EasyOCR do not suffer that problem as much. But tesseract as the advantage of not relying on pytorch and running fine on CPU. Consequently, we plan on attempting to keep tesseract as a backend, but add some typo fixers.</p>"},{"location":"reference/","title":"Welcome to <code>chunknorris</code> API reference","text":""},{"location":"reference/chunkers/chunker_markdown/","title":"Reference for <code>MarkdownChunker</code>","text":"<p>The MarkdownChunker is used to process the documents parsed to a <code>MarkdownDoc</code> object.</p> <p>               Bases: <code>AbstractChunker</code></p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.__init__","title":"<code>__init__(max_headers_to_use='h4', max_chunk_word_count=200, hard_max_chunk_word_count=400, min_chunk_word_count=15, hard_max_chunk_token_count=None, tokenizer=None)</code>","text":"<p>Initialize a Markdown chunker</p> <p>Parameters:</p> Name Type Description Default <code>max_headers_to_use (MaxHeadersToUse) </code> <p>The maximum header level to consider (included). Headers with level lower than this wont be used to split chunks. For example, if 'h4' is set, then 'h5' and 'h6' headers won't be used. Must be a string of type 'hx' with x being the title level ranging from 1 to 6.</p> required <code>max_chunk_word_count (int) </code> <p>The maximum size a chunk can be (in words). It is a SOFT limit, meaning that chunks bigger that this will be chunked only if lower level headers if any are available.\"</p> required <code>hard_max_chunk_word_count (int) </code> <p>The true maximum size a chunk can be (in word). It is a HARD limit, meaning that chunks bigger by this limit will be split into subchunks.</p> required <code>min_chunk_word_count (int) </code> <p>The minimum size a chunk can be (in words). Chunks smaller than this will be discarded.</p> required <code>hard_max_chunk_token_count (None | int) </code> <p>The true maximum size a chunk can be (in tokens). If None, no token-based splitting will be done. It is a HARD limit, meaning that chunks bigger by this limit will be split into subchunks that are equivalent in terms of tokens count.</p> required <code>tokenizer (Any | None) </code> <p>The tokenizer to use. Can be any instance of a class that has 'encode' method such as tiktoken.</p> required"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.build_chunks","title":"<code>build_chunks(toc_tree_element, already_ok_chunks=None)</code>","text":"<p>Uses the toc tree to build the chunks. Uses recursion. Method : - build the chunk (= titles from sections above + section content + content of subsections) - if the chunk is too big:     - save the section as title + content (if section has content)     - subdivide section recursively using subsections - else save it as is</p> <p>Parameters:</p> Name Type Description Default <code>toc_tree_element</code> <code>TocTree</code> <p>the TocTree for which the chunk should be build</p> required <code>already_ok_chunks</code> <code>Chunks</code> <p>the chunks already built. Used for recursion. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Chunks</code> <code>list[Chunk]</code> <p>list of chunk's texts.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.chunk","title":"<code>chunk(content)</code>","text":"<p>Chunks a parsed Markdown document.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>MarkdownDoc</code> <p>the markdown document to chunk. Might be the output of a chunknorris.Parser.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: the chunks.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.get_chunks","title":"<code>get_chunks(toc_tree)</code>","text":"<p>Wrapper that build the chunk's texts, check that they fit in size, replace links formatting.</p> <p>Parameters:</p> Name Type Description Default <code>toc_tree</code> <code>TocTree</code> <p>the toc tree of the document.</p> required <p>Returns:</p> Name Type Description <code>Chunks</code> <code>list[Chunk]</code> <p>the chunks text, formatted.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.get_parents_headers","title":"<code>get_parents_headers(toc_tree_element)</code>  <code>staticmethod</code>","text":"<p>Gets a list of the titles that are parent of the provided toc tree element. The list is ordered in descending order in terms of header level.</p> <p>Parameters:</p> Name Type Description Default <code>toc_tree_element</code> <code>TocTree</code> <p>the toc tree element.</p> required <p>Returns:</p> Type Description <code>list[MarkdownLine]</code> <p>list[MarkdownLine]: the list of line that represent the parent's headers.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.get_toc_tree","title":"<code>get_toc_tree(md_lines)</code>","text":"<p>Builds the table of content tree based on header.</p> <p>Parameters:</p> Name Type Description Default <code>md_lines</code> <code>list[MarkdownLines]</code> <p>the markdown lines.</p> required <p>Returns:</p> Name Type Description <code>TocTree</code> <code>TocTree</code> <p>the table of content.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.remove_small_chunks","title":"<code>remove_small_chunks(chunks)</code>","text":"<p>Removes chunks that have less words than the specified limit.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>Chunks</code> <p>the list of chunks.</p> required <p>Returns:</p> Name Type Description <code>Chunks</code> <code>list[Chunk]</code> <p>the chunks with more words than the specified threshold.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.split_big_chunks_tokenbased","title":"<code>split_big_chunks_tokenbased(chunks)</code>","text":"<p>Splits the chunks that are too big considering the provided tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Chunk]</code> <p>the chunks to split.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the tokenizer is not provided.</p> <code>ValueError</code> <p>if the tokenizer does not have 'encode' method.</p> <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: the chunks, with big chunks splitting into smaller chunks.</p>"},{"location":"reference/chunkers/chunker_markdown/#chunknorris.chunkers.markdown_chunker.MarkdownChunker.split_big_chunks_wordbased","title":"<code>split_big_chunks_wordbased(chunks)</code>","text":"<p>Splits the chunks that are too big. You may consider passing the kwarg \"hard_max_chunk_word_count\" to specify the limit size of the chunk (in words).</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>Chunks</code> <p>The chunks obtained from the get_chunks() method.</p> required <p>Returns:</p> Name Type Description <code>Chunks</code> <code>list[Chunk]</code> <p>the chunks, with big chunks splitted into smaller chunks.</p>"},{"location":"reference/components/component_chunk/","title":"Reference for <code>Chunk</code>","text":"<p>The <code>Chunk</code> is the entity returned by <code>chunknorris</code>'s chunkers. It contains various elements related to the chunks : it's text content, headers, the pages it comes from (if from paginated documents) etc.  You might essentially need to use <code>Chunk.get_text()</code> to get the cleaned chunk's content as text preceded by its headers. </p> <p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/components/component_chunk/#chunknorris.core.components.Chunk.word_count","title":"<code>word_count: int</code>  <code>property</code>","text":"<p>Gets the amount of words in the chunk's content (headers not included)</p>"},{"location":"reference/components/component_chunk/#chunknorris.core.components.Chunk.get_text","title":"<code>get_text(remove_links=False, prepend_headers=True)</code>","text":"<p>Gets the text of the chunk.</p> <p>Parameters:</p> Name Type Description Default <code>remove_links</code> <code>bool</code> <p>If True, the markdown links will be removed (text of the link is kept). Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the text</p>"},{"location":"reference/components/component_chunk/#chunknorris.core.components.Chunk.remove_links","title":"<code>remove_links(text)</code>  <code>staticmethod</code>","text":"<p>Removes the markdown format of the links in the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text to find the links in</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the formated text</p>"},{"location":"reference/components/component_markdowndoc/","title":"Reference for <code>MarkdownDoc</code>","text":"<p>The <code>MarkdownDoc</code> is the entity returned by <code>chunknorris</code>'s parsers. It's purpose it mainly to be fed to the <code>MarkdownChunker</code>.</p> <p>               Bases: <code>BaseModel</code></p> <p>A parsed Markdown Formatted-String, resulting in a list of MarkdownLine. Feats : - ATX header formatting. - Remove base64 images</p>"},{"location":"reference/components/component_markdowndoc/#chunknorris.core.components.MarkdownDoc.from_string","title":"<code>from_string(md_string)</code>  <code>staticmethod</code>","text":"<p>Get the MardownDoc object from a markdown formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>the markdown string</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the markdown document</p>"},{"location":"reference/components/component_markdowndoc/#chunknorris.core.components.MarkdownDoc.to_string","title":"<code>to_string()</code>","text":"<p>Get the markdown string corresponding to the document's content</p>"},{"location":"reference/components/component_markdownline/","title":"Reference for <code>MarkdownLine</code>","text":"<p>The <code>MarkdownLine</code> represents a markdown-formatted line and some associated features (whether the line is a header, or belongs to a code block for example).</p> <p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/components/component_markdownline/#chunknorris.core.components.MarkdownLine.is_bullet_point","title":"<code>is_bullet_point: bool</code>  <code>property</code>","text":"<p>whether or not the line is a bullet point</p>"},{"location":"reference/components/component_markdownline/#chunknorris.core.components.MarkdownLine.isin_table","title":"<code>isin_table: bool</code>  <code>property</code>","text":"<p>whether or not the line belongs to a table</p>"},{"location":"reference/components/component_markdownline/#chunknorris.core.components.MarkdownLine.get_header_level","title":"<code>get_header_level()</code>","text":"<p>Gets the header level of this line (1-based)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the line is not a header, raises an error</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the header level, h1 headers would return 1</p>"},{"location":"reference/parsers/parser_csv/","title":"Reference for <code>CSVParser</code>","text":"<p>The <code>CSVParser</code> is dedicated to the parsing of comma-separated Value file (.csv). By default it will attempt to infer the delimiter used (comma, semicolon, ...). Otherwise you may specify the delimiter it should use.</p> <p>               Bases: <code>AbstractParser</code></p> <p>Parser for Comma-Separated Values file (.csv)</p>"},{"location":"reference/parsers/parser_csv/#chunknorris.parsers.sheets.csv_parser.CSVParser.__init__","title":"<code>__init__(csv_delimiter=None, output_format='json_lines')</code>","text":"<p>Initializes a sheet parser</p> <p>Parameters:</p> Name Type Description Default <code>csv_delimiter</code> <code>str | None</code> <p>The delimiter to consider to parse the .csv files. If None, we will try to guess what the delimiter is. Defaults to None.</p> <code>None</code> <code>output_format</code> <code>Literal[&amp;quot;markdown_table&amp;quot;, &amp;quot;json_lines&amp;quot;]</code> <p>the output format of the parsed document. - markdown_table : uses tabula to build a markdown-formatted table. - json_lines : each row of the table will be output as a JSON line. NOTE : consumes way more tokens as column names are repeated at each row. But easier to read for LLMs. Defaults to \"json_lines\".</p> <code>'json_lines'</code>"},{"location":"reference/parsers/parser_csv/#chunknorris.parsers.sheets.csv_parser.CSVParser.convert_df_to_json_lines","title":"<code>convert_df_to_json_lines(df)</code>  <code>staticmethod</code>","text":"<p>Converts a DataFrame to json lines.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the dataframe to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the json lines.</p>"},{"location":"reference/parsers/parser_csv/#chunknorris.parsers.sheets.csv_parser.CSVParser.convert_df_to_markdown_table","title":"<code>convert_df_to_markdown_table(df)</code>  <code>staticmethod</code>","text":"<p>Converts a DataFrame to markdown.        Wraps tabula's method pd.DataFrame.to_markdown()        between pre and post processing.        Preprocess :        - Remove  in text columns        PostProcess :        - Replace multiple spaces with 2 spaces.</p> <pre><code>   Args:\n       df (pd.DataFrame): the dataframe to convert.\n\n   Returns:\n       str: a markdown formatted table.\n</code></pre>"},{"location":"reference/parsers/parser_csv/#chunknorris.parsers.sheets.csv_parser.CSVParser.parse_file","title":"<code>parse_file(filepath)</code>","text":"<p>Parses a csv file to markdown.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the csv file.</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the markdown-formatted csv.</p>"},{"location":"reference/parsers/parser_csv/#chunknorris.parsers.sheets.csv_parser.CSVParser.parse_string","title":"<code>parse_string(string)</code>","text":"<p>Parses a string representing a csv file to markdown.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>the csv-formatted string.</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the markdown-formatted csv.</p>"},{"location":"reference/parsers/parser_csv/#chunknorris.parsers.sheets.csv_parser.CSVParser.read_file","title":"<code>read_file(filepath)</code>","text":"<p>Read the provided filepath. For a list of handled filetypes, refer to https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the csv file content as a string.</p>"},{"location":"reference/parsers/parser_docx/","title":"Reference for <code>DocxParser</code>","text":"<p>               Bases: <code>HTMLParser</code></p>"},{"location":"reference/parsers/parser_docx/#chunknorris.parsers.docx.docx_parser.DocxParser.parse_file","title":"<code>parse_file(filepath)</code>","text":"<p>Reads and parses a markdown-formatted string. Ensures that the formatting is suited to be passed to the MarkdownChunker.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>FilePath</code> <p>the path to a .html file</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the parsed document. Can be fed to chunker.</p>"},{"location":"reference/parsers/parser_docx/#chunknorris.parsers.docx.docx_parser.DocxParser.parse_string","title":"<code>parse_string(string)</code>","text":"<p>Parses a HTML-formatted string. Ensures that the formatting is suited to be passed to the MarkdownChunker.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>the markdown formatted string</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the parsed document. Can be fed to chunker.</p>"},{"location":"reference/parsers/parser_docx/#chunknorris.parsers.docx.docx_parser.DocxParser.read_file","title":"<code>read_file(filepath)</code>  <code>staticmethod</code>","text":"<p>Reads a Markdown file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the HTML file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the HTML string.</p>"},{"location":"reference/parsers/parser_excel/","title":"Reference for <code>ExcelParser</code>","text":"<p>The <code>ExcelParser</code> enables parsing spreadsheets, such as .xslx files. All sheets in the notebook will be parsed.</p> <p>               Bases: <code>AbstractParser</code></p> <p>Parser for spreadsheets, such as Excel workbooks (.xslx). For a list of handled filetypes, refer to https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html</p>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.__init__","title":"<code>__init__(output_format='auto')</code>","text":"<p>Initializes an Excel parser</p> <p>Parameters:</p> Name Type Description Default <code>output_format</code> <code>Literal[\"markdown_table\", \"json_lines\", \"auto]</code> <p>the output format of the parsed document. - markdown_table : uses tabula to build a markdown-formatted table. - json_lines : each row of the table will be output as a JSON line. Better for chunking as headers are preserved. - auto : will detect which format is the more suitable. CSV-like sheet will be converset to JSON lines. Defaults to \"auto\".</p> <code>'auto'</code>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.convert_df_to_json_lines","title":"<code>convert_df_to_json_lines(df)</code>  <code>staticmethod</code>","text":"<p>Converts a DataFrame to json lines.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the dataframe to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the json lines.</p>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.convert_df_to_markdown_table","title":"<code>convert_df_to_markdown_table(df)</code>  <code>staticmethod</code>","text":"<p>Converts a DataFrame to markdown.        Wraps tabula's method pd.DataFrame.to_markdown()        between pre and post processing.        Preprocess :        - Remove  in text columns        PostProcess :        - Replace multiple spaces with 2 spaces.</p> <pre><code>   Args:\n       df (pd.DataFrame): the dataframe to convert.\n\n   Returns:\n       str: a markdown formatted table.\n</code></pre>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.convert_sheets_to_output_format","title":"<code>convert_sheets_to_output_format(sheets)</code>","text":"<p>Handle the conversion of the sheets obtained from pandas.read_excel() method to the specified output format.</p> <p>Parameters:</p> Name Type Description Default <code>sheets</code> <code>dict[str, DataFrame]</code> <p>the sheets returned from pd.read_excel(sheet_name=None).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the formatted string</p>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.parse_file","title":"<code>parse_file(filepath)</code>","text":"<p>Parses a excel-like file to markdown.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the excel-like file.</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the markdown formatted excel file.</p>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.parse_string","title":"<code>parse_string(string)</code>","text":"<p>Parses a bytes string representing an excel file.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>bytes</code> <p>the excel as a byte string.</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the markdown formatted excel file</p>"},{"location":"reference/parsers/parser_excel/#chunknorris.parsers.sheets.excel_parser.ExcelParser.read_file","title":"<code>read_file(filepath)</code>","text":"<p>Read the provided filepath.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the file.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, pd.DataFrame]: a mapping containing {sheet_name: corresponding_dataframe.}</p>"},{"location":"reference/parsers/parser_html/","title":"Reference for <code>HTMLParser</code>","text":"<p>               Bases: <code>AbstractParser</code></p>"},{"location":"reference/parsers/parser_html/#chunknorris.parsers.html.html_parser.HTMLParser.apply_markdownify","title":"<code>apply_markdownify(html_string)</code>  <code>staticmethod</code>","text":"<p>Applies markdownify to the html text</p> <p>Parameters:</p> Name Type Description Default <code>html_string</code> <code>str</code> <p>an HTML-formatted string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the markdownified string.</p>"},{"location":"reference/parsers/parser_html/#chunknorris.parsers.html.html_parser.HTMLParser.cleanup_string","title":"<code>cleanup_string(md_string)</code>  <code>staticmethod</code>","text":"<p>Cleans up the html string.</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>the markdown string, output from apply_markdownify()</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the cleaned up string.</p>"},{"location":"reference/parsers/parser_html/#chunknorris.parsers.html.html_parser.HTMLParser.parse_file","title":"<code>parse_file(filepath)</code>","text":"<p>Reads and parses a markdown-formatted string. Ensures that the formatting is suited to be passed to the MarkdownChunker.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>FilePath</code> <p>the path to a .html file</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the parsed document. Can be fed to chunker.</p>"},{"location":"reference/parsers/parser_html/#chunknorris.parsers.html.html_parser.HTMLParser.parse_string","title":"<code>parse_string(string)</code>","text":"<p>Parses a markdown-formatted string. Ensures that the formatting is suited to be passed to the MarkdownChunker.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>the markdown formatted string</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the parsed document. Can be fed to chunker.</p>"},{"location":"reference/parsers/parser_html/#chunknorris.parsers.html.html_parser.HTMLParser.read_file","title":"<code>read_file(filepath)</code>  <code>staticmethod</code>","text":"<p>Reads a Markdown file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the HTML file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the HTML string.</p>"},{"location":"reference/parsers/parser_jupyter_notebook/","title":"Reference for <code>JupyterNotebookParser</code>","text":"<p>The <code>JupyterNotebookParser</code> enables parsing of jupyter notebooks (.ipynb files).</p> <p>               Bases: <code>AbstractParser</code></p> <p>Class used to parse jupyter notebooks (.ipynb).</p>"},{"location":"reference/parsers/parser_jupyter_notebook/#chunknorris.parsers.notebook.jupyter_notebook_parser.JupyterNotebookParser.__init__","title":"<code>__init__(include_code_cells_outputs=False)</code>","text":"<p>Initializes the class.</p> <p>Parameters:</p> Name Type Description Default <code>include_code_cells_outputs (bool) </code> <p>Whether or not the cells' output should be parsed as well. If False, they will be ignored. Default to False.</p> required"},{"location":"reference/parsers/parser_jupyter_notebook/#chunknorris.parsers.notebook.jupyter_notebook_parser.JupyterNotebookParser.parse_file","title":"<code>parse_file(filepath)</code>","text":"<p>Chunks a notebook .ipynb file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the file to parse.</p> required <p>Returns:</p> Name Type Description <code>MarkDownDoc</code> <code>MarkdownDoc</code> <p>the parsed markdown object.</p>"},{"location":"reference/parsers/parser_jupyter_notebook/#chunknorris.parsers.notebook.jupyter_notebook_parser.JupyterNotebookParser.parse_string","title":"<code>parse_string(string)</code>","text":"<p>Parses the string considering it is a notebook content.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>the string representing the notebook content. Assumed to be a dumped version of the JSON formatted notebook.</p> required <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>the formatted markdown document.</p>"},{"location":"reference/parsers/parser_jupyter_notebook/#chunknorris.parsers.notebook.jupyter_notebook_parser.JupyterNotebookParser.read_file","title":"<code>read_file(filepath)</code>  <code>staticmethod</code>","text":"<p>Reads a .ipynb file and returns its content as a json dict.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the file</p> required <p>Returns:</p> Type Description <code>JupyterNotebookContent</code> <p>dict[str, Any]: the json content of the ipynb file</p>"},{"location":"reference/parsers/parser_markdown/","title":"Reference for <code>MarkdownParser</code>","text":"<p>The <code>MarkdownParser</code> is used to process markdown-formatted string or files to a <code>MarkdownDoc</code> object that can be fed to a chunker. It will ensure that the markdown formatting is as expected by the chunker (ATX heading style, parsing of metadata, etc...).</p> <p>               Bases: <code>AbstractParser</code></p>"},{"location":"reference/parsers/parser_markdown/#chunknorris.parsers.markdown.markdown_parser.MarkdownParser.cleanup_string","title":"<code>cleanup_string(md_string)</code>  <code>staticmethod</code>","text":"<p>Cleans up the html string.</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>the markdown string, output from apply_markdownify()</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the cleaned up string.</p>"},{"location":"reference/parsers/parser_markdown/#chunknorris.parsers.markdown.markdown_parser.MarkdownParser.convert_setext_to_atx","title":"<code>convert_setext_to_atx(md_string)</code>  <code>staticmethod</code>","text":"<p>Converts headers from setext style to atx style</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>the markdown string</p> required Return <p>str: the string with formatted headers</p>"},{"location":"reference/parsers/parser_markdown/#chunknorris.parsers.markdown.markdown_parser.MarkdownParser.parse_file","title":"<code>parse_file(filepath)</code>","text":"<p>Reads and parses a markdown-formatted string. Ensures that the formatting is suited to be passed to the MarkdownChunker.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>FilePath</code> <p>the path to a .md file</p> required <p>Returns:</p> Name Type Description <code>TypedString</code> <code>MarkdownDoc</code> <p>the typed string</p>"},{"location":"reference/parsers/parser_markdown/#chunknorris.parsers.markdown.markdown_parser.MarkdownParser.parse_metadata","title":"<code>parse_metadata(md_string)</code>  <code>staticmethod</code>","text":"<p>Parses the metadatas of a markdown string. Assumes the metadata are in YAML format, with '---' as first line. Example : <pre><code>---\nmetakey : metavalue\n---\n\nContent of document...\n</code></pre> Args:     md_string (str): the string to get the metadata from</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the content of the docu, with the metadata section removed</p> <code>dict[str, Any]</code> <p>dict[str, Any]: the parsed metadata, as dict</p>"},{"location":"reference/parsers/parser_markdown/#chunknorris.parsers.markdown.markdown_parser.MarkdownParser.parse_string","title":"<code>parse_string(string)</code>","text":"<p>Parses a markdown-formatted string. Ensures that the formatting is suited to be passed to the MarkdownChunker.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>the markdown formatted string</p> required <p>Returns:</p> Name Type Description <code>TypedString</code> <code>MarkdownDoc</code> <p>the formatted markdown string</p>"},{"location":"reference/parsers/parser_markdown/#chunknorris.parsers.markdown.markdown_parser.MarkdownParser.read_file","title":"<code>read_file(filepath)</code>  <code>staticmethod</code>","text":"<p>Reads a Markdown file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the markdown file</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the markdown string</p>"},{"location":"reference/parsers/parser_pdf/","title":"Reference for <code>PdfParser</code>","text":"<p>The <code>PdfParser</code>'s functionnalities are splitted among various classes. In the end, <code>PdfParser</code> wraps those functionnalities into a document parsing pipeline.</p> <p>               Bases: <code>PdfLinkExtraction</code>, <code>PdfTableExtraction</code>, <code>PdfTocExtraction</code>, <code>PdfPlotter</code>, <code>PdfExport</code>, <code>DocSpecsExtraction</code>, <code>PdfParserState</code></p> <p>Class that parses the document.</p>"},{"location":"reference/parsers/parser_pdf/#chunknorris.parsers.pdf.pdf_parser.PdfParser.__init__","title":"<code>__init__(*, extract_tables=True, table_finder=TableFinder(), add_headers=True, use_ocr='auto', ocr_language='fra+eng', body_line_spacing=None)</code>","text":"<p>Initializes a pdf parser.</p> <p>Parameters:</p> Name Type Description Default <code>extract_tables</code> <code>bool</code> <p>whether or not tables should be extracted. Defaults to True.</p> <code>True</code> <code>add_headers</code> <code>bool</code> <p>if True, the parser will try to find a table of content. either in documents or in metadata and style the headers accordingly. Defaults to True.</p> <code>True</code> <code>use_ocr</code> <code>str</code> <p>whether or not OCR should be used. Allows to detect text on images but keep in mind that this might include text you actually do not want, such as screenshots. Must be one of [\"always\", \"auto\", \"never\"]. Default to \"auto\".</p> <code>'auto'</code> <code>ocr_language (str, optional) </code> <p>the languages to consider for OCR. Must be a string of 3 letter codes languages separated by \"+\". Example : \"fra+eng+ita\"</p> required <code>body_line_spacing (float, optional) </code> <p>the size of the space between 2 lines of the body of the document. Generally around 1. If None, an automatic method will try to find it. Tweak this parameter for better merging of lines into blocks.</p> required <code>table_finder</code> <code>TableFinder | None</code> <p>the table finder to use for parsing the tables. If None, defauts to a TableFinder with default parameters.</p> <code>TableFinder()</code>"},{"location":"reference/parsers/parser_pdf/#chunknorris.parsers.pdf.pdf_parser.PdfParser.check_ocr_config_is_valid","title":"<code>check_ocr_config_is_valid()</code>","text":"<p>Check that the OCR configuration is valid.</p>"},{"location":"reference/parsers/parser_pdf/#chunknorris.parsers.pdf.pdf_parser.PdfParser.cleanup_memory","title":"<code>cleanup_memory()</code>","text":"<p>Cleans up memory by reseting all objects created to parse the document.</p>"},{"location":"reference/parsers/parser_pdf/#chunknorris.parsers.pdf.pdf_parser.PdfParser.parse_file","title":"<code>parse_file(filepath, page_start=0, page_end=None)</code>","text":"<p>Parses a pdf document and returns the parsed MarkdownDoc object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>the path to the file to parse.</p> required <code>page_start</code> <code>int</code> <p>the page to start parsing from. Defaults to 0.</p> <code>0</code> <code>page_end</code> <code>int</code> <p>the page to stop parsing. None to parse until last page. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>The MarkdownDoc to be passed to MarkdownChunker.</p>"},{"location":"reference/parsers/parser_pdf/#chunknorris.parsers.pdf.pdf_parser.PdfParser.parse_string","title":"<code>parse_string(string, page_start=0, page_end=None)</code>","text":"<p>Parses a byte string obtained from a pdf document and returns its corresponding Markdown formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>bytes</code> <p>a bytes stream.</p> required <code>page_start</code> <code>int</code> <p>the page to start parsing from. Defaults to 0.</p> <code>0</code> <code>page_end</code> <code>int</code> <p>the page to stop parsing. None to parse until last page. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MarkdownDoc</code> <code>MarkdownDoc</code> <p>The MarkdownDoc to be passed to MarkdownChunker.</p>"},{"location":"reference/pipelines/pipeline_base/","title":"Reference for <code>BasePipeline</code>","text":"<p>As its name suggests, the <code>BasePipeline</code> will simply feed the output of the provided parser as input to the provided chunker.</p> <p>               Bases: <code>AbstractPipeline</code></p>"},{"location":"reference/pipelines/pipeline_base/#chunknorris.pipelines.base_pipeline.BasePipeline.chunk_file","title":"<code>chunk_file(filepath)</code>","text":"<p>Parses and chunks a string based on the provided parser and chunker.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Filepath</code> <p>the path to a file.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: A list of chunks.</p>"},{"location":"reference/pipelines/pipeline_base/#chunknorris.pipelines.base_pipeline.BasePipeline.chunk_string","title":"<code>chunk_string(string)</code>","text":"<p>Parses and chunks a string based on the provided parser and chunker.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>a string.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: A list of chunks.</p>"}]}