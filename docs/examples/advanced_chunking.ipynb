{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install chunknorris\n",
    "%pip install chunknorris -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def print_chunking_result(chunks):\n",
    "    print(f\"\\n======= Got {len(chunks)} chunks ! ========\\n\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"--------------------- chunk {i} ---------------------\")\n",
    "        print(chunk.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence chunking behavior\n",
    "One may want to influence how the chunks are built by passing parameters to the ``MarkdownChunker``. This notebook intends to git a feeling of \"which parameter does what\". Happy chunking ! ðŸ”ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunknorris.parsers import MarkdownParser # <- you can use any parser you want as long as the are compatible with MarkdownChunker\n",
    "from chunknorris.chunkers import MarkdownChunker # <- tutorial is essentially about this guy\n",
    "from chunknorris.pipelines import BasePipeline\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will consider this easy Markdown :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# This is header 1\n",
       "\n",
       "This is some intruction text after header 1\n",
       "\n",
       "## This is SUBheader 1\n",
       "\n",
       "This is some intruction text after header 1\n",
       "\n",
       "### This is an h3 header\n",
       "\n",
       "This is the content of the h3 subsection\n",
       "\n",
       "### This is ANOTHER h3 header\n",
       "\n",
       "This is the other content of the h3 subsection\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_string = \"\"\"\n",
    "# This is header 1\n",
    "\n",
    "This is some intruction text after header 1\n",
    "\n",
    "## This is SUBheader 1\n",
    "\n",
    "This is some intruction text after header 1\n",
    "\n",
    "### This is an h3 header\n",
    "\n",
    "This is the content of the h3 subsection\n",
    "\n",
    "### This is ANOTHER h3 header\n",
    "\n",
    "This is the other content of the h3 subsection\n",
    "\"\"\"\n",
    "Markdown(md_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0001 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 1 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "# Pipeline with default parameters:\n",
    "pipeline = BasePipeline(MarkdownParser(), MarkdownChunker())\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of each argument\n",
    "\n",
    "### max_chunk_word_count\n",
    "\n",
    "We can see that we got only one chunk, despite the presence of headers !\n",
    "\n",
    "This is due to ``MarkdownChunker``'s parameter ``max_chunk_word_count``. The default value is ``200``, meaning that the chunker will try to make chunks of approximately 200 words. I a chunk is bigger that this, only then it will be chunked using its header.\n",
    "\n",
    "This may sound weird, but embedding models are still sensitive to the length of the text. Consequently, \"I have a dog\" may be more similar to \"I love electronic music\" than a whole paragraph about dogs. **ðŸ’¡ By ensuring the resulting chunks are of similar sizes, we minimize the influence of the chunk's size in the embedding and make it more about the chunk's meaning.**\n",
    "\n",
    "Let's play with ``max_chunk_word_count`` a bit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0002 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 2 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 1 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "chunker = MarkdownChunker(\n",
    "    max_chunk_word_count=50, \n",
    "    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n",
    "    )\n",
    "pipeline = BasePipeline(MarkdownParser(), chunker)\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with parameter ``max_chunk_word_count=50`` the chunk is introduction part is split from the rest to make sure both chunks are below 50 words. We could even decrease that number to split the second chunk even more.\n",
    "\n",
    "ðŸ’¡ **Pro tip : Want to make sure all the headers are used to build chunks ?** Just set ``max_chunk_word_count=0`` and the chunker will try to make chunks of 0 word count, hence using all headers available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0002 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 4 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 1 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 2 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "--------------------- chunk 3 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "chunker = MarkdownChunker(\n",
    "    max_chunk_word_count=0, \n",
    "    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n",
    "    )\n",
    "pipeline = BasePipeline(MarkdownParser(), chunker)\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_headers_to_use\n",
    "\n",
    "You may want to get chunks as small as possible, but avoid using headers level that are too low. Indeed, it is common that list-items in html are h5 headers and you wouldn't want each item in the list ot be a chunk. \n",
    "\n",
    "**By default, MarkdownChunker will only use headers up to ``H4``**, and won't use h5 and h6. But let's change this and see how it affects behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0001 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 2 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 1 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "chunker = MarkdownChunker(\n",
    "    max_chunk_word_count=0,\n",
    "    max_headers_to_use=\"h2\", # <- only use h1 and h2 to split chunks\n",
    "    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n",
    "    )\n",
    "pipeline = BasePipeline(MarkdownParser(), chunker)\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have 2 chunks, as h3 headers were not allowed to be used to split the chunks.\n",
    "\n",
    "### hard_max_chunk_word_count\n",
    "\n",
    "Now, we saw that forbidding the ``MarkdownChunker`` to use headers will enforce chunks to be bigger that that what is requested by the ``max_chunk_word_count`` parameter. The same happens if no header is available in the markdown : the chunker will try to make chunks of requested size, but if no header is available it will leave the chunk \"as is\".\n",
    "\n",
    "This may lead to veeeery big chunks and we don't want that (and most embedding API will trigger an error if your chunk is bigger than the model's context window).\n",
    "\n",
    "That's when the ``hard_max_chunk_word_count`` comes into play. This parameter allows you to set a *kind of* hard limit for the chunk. Chunks bigger that the limit will be splitted to fit the limit. Why *kind of* ? Because **``MarkdownChunker`` will avoid splitting in the middle of a code block, or in a table** so **you may still have resulting chunks that are slightly bigger** that this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:09:ChunkNorris:INFO:Function \"chunk\" took 0.0002 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 3 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 1 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "--------------------- chunk 2 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "chunker = MarkdownChunker(\n",
    "    max_chunk_word_count=0,\n",
    "    max_headers_to_use=\"h2\", # <- only use h1 and h2 to split chunks\n",
    "    hard_max_chunk_word_count=20,\n",
    "    min_chunk_word_count=0 # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n",
    "    )\n",
    "pipeline = BasePipeline(MarkdownParser(), chunker)\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, the second chunk from before has been splitted in 2 !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hard_max_chunk_token_count and tokenizer\n",
    "\n",
    "This parameter allow to set an actual hard limit in terms of token to avoid any errors regarding API calls to embedding model providers. \n",
    "\n",
    "When ```hard_max_chunk_token_count``` is set to an *int* value, the provided ```tokenizer``` will be used to count tokens. Chunks bigger than the value specified will be split into subchunks, trying to equilibrate their size and considering newlines to avoid random cuts.\n",
    "\n",
    "The provided tokenizer MUST have an ``encode(str) -> list[int]`` method, that takes a string as input, and returns a list of tokens. For example, the tiktoken package nativiely have such method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:10:ChunkNorris:INFO:Function \"chunk\" took 0.0005 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 4 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 1 ---------------------\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "--------------------- chunk 2 ---------------------\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "--------------------- chunk 3 ---------------------\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "chunker = MarkdownChunker(\n",
    "    min_chunk_word_count=0, # we set this to 0 because the chunker automatically discard chunks below 15 words by default\n",
    "    hard_max_chunk_token_count=20,\n",
    "    tokenizer=tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "    )\n",
    "pipeline = BasePipeline(MarkdownParser(), chunker)\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_chunk_word_count\n",
    "\n",
    "When all this chunking is happening, you may have small chunks remaining. This is frequent for webscrapping for example, where some pages will be empty or just have a title.\n",
    "\n",
    "As said before, these small chunk can be a pain because in the context of information retrieval based on queries thay tend to come up because of their similar size with the queries. \n",
    "\n",
    "To avoid having these small chunks lost in a database full of great chunks, the ``min_chunk_word_count`` argument is here. This will allow chunks will less words than the limit to be automatically discarded. The default value is ``15`` but you may to set it to 0 if you absolutely wish to keep every chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 10:59:ChunkNorris:INFO:Function \"chunk\" took 0.0003 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Got 2 chunks ! ========\n",
      "\n",
      "--------------------- chunk 0 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "This is some intruction text after header 1\n",
      "\n",
      "### This is an h3 header\n",
      "\n",
      "This is the content of the h3 subsection\n",
      "--------------------- chunk 1 ---------------------\n",
      "# This is header 1\n",
      "\n",
      "## This is SUBheader 1\n",
      "\n",
      "### This is ANOTHER h3 header\n",
      "\n",
      "This is the other content of the h3 subsection\n"
     ]
    }
   ],
   "source": [
    "chunker = MarkdownChunker(\n",
    "    max_chunk_word_count=0,\n",
    "    max_headers_to_use=\"h2\",\n",
    "    hard_max_chunk_word_count=20,\n",
    "    min_chunk_word_count=10 # Discard chunks will less than 10 words (excluding headers)\n",
    "    )\n",
    "pipeline = BasePipeline(MarkdownParser(), chunker)\n",
    "chunks = pipeline.chunk_string(md_string)\n",
    "print_chunking_result(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go. The first chunk from before has been dicarded.\n",
    "\n",
    "## Work with the TOC tree\n",
    "\n",
    "To build the chunks according to the markdown headers, the MarkdownChunker uses a ``TocTree`` object. The ``TocTree`` represents the table of content, and the content of each part.\n",
    "\n",
    "Whether it is for **debugging**, or because you want to **implement some  custom chunking strategy**, you may want to have a look at the table of content that has been parsed from your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MarkdownDoc that can be fed to the chunker\n",
    "parser = MarkdownParser()\n",
    "md_doc = parser.parse_string(md_string)\n",
    "\n",
    "# New, get the TocTree\n",
    "chunker = MarkdownChunker()\n",
    "toc_tree = chunker.get_toc_tree(md_doc.content)\n",
    "\n",
    "# Save the TocTree to have a look at it\n",
    "toc_tree.to_json(output_path=\"toc_tree.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "ðŸ§ª Feel free to experiments with these parameters to get the chunks that suit your data.\n",
    "\n",
    "If this still seem a bit obscure, don't worry the default parameters have been tested on multiple custom dataset and have proven to work well ðŸ˜‰ !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunknorris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
